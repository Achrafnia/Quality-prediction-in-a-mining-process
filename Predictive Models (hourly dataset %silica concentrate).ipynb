{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('MiningProcess_Flotation_Plant_Database_hour.csv')\n",
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['% Iron Concentrate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['% Silica Concentrate']\n",
    "X = df.drop(['% Silica Concentrate'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalisation using : min_max_scaler ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = pd.DataFrame(min_max_scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting Data to train and test ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Cross Validation     (K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=True)\n",
      "TRAIN: [   1    2    3 ... 3272 3274 3276] TEST: [   0    4    9 ... 3271 3273 3275]\n",
      "TRAIN: [   0    1    2 ... 3273 3275 3276] TEST: [   5   11   14 ... 3269 3272 3274]\n",
      "TRAIN: [   0    4    5 ... 3273 3274 3275] TEST: [   1    2    3 ... 3264 3268 3276]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "kf.get_n_splits(X_train)\n",
    "print(kf)\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train3, X_test3 = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "     Y_train3, Y_test3 = Y_train.iloc[train_index], Y_train.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': False}\n"
     ]
    }
   ],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(lr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14746547, 0.18451596, 0.14592256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr, X_train, Y_train, scoring='r2', cv=kf)\n",
    "scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify range of hyperparameters to tune\n",
    "hyper_params = [{'n_features_to_select': list(range(2, 40))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify model\n",
    "from sklearn.feature_selection import RFE\n",
    "lr.fit(X_train, Y_train)\n",
    "rfe = RFE(lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 114 out of 114 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=True),\n",
       "             estimator=RFE(estimator=LinearRegression()),\n",
       "             param_grid=[{'n_features_to_select': [2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                                   11, 12, 13, 14, 15, 16, 17,\n",
       "                                                   18, 19, 20, 21, 22, 23, 24,\n",
       "                                                   25, 26, 27, 28, 29, 30, 31, ...]}],\n",
       "             return_train_score=True, scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = rfe, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = kf, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train, Y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_features_to_select</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073271</td>\n",
       "      <td>0.008882</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_features_to_select': 2}</td>\n",
       "      <td>0.082294</td>\n",
       "      <td>0.102248</td>\n",
       "      <td>0.071757</td>\n",
       "      <td>0.085433</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>38</td>\n",
       "      <td>0.097624</td>\n",
       "      <td>0.085776</td>\n",
       "      <td>0.102462</td>\n",
       "      <td>0.095287</td>\n",
       "      <td>0.007009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069120</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_features_to_select': 3}</td>\n",
       "      <td>0.122841</td>\n",
       "      <td>0.128139</td>\n",
       "      <td>0.104549</td>\n",
       "      <td>0.118510</td>\n",
       "      <td>0.010106</td>\n",
       "      <td>37</td>\n",
       "      <td>0.120339</td>\n",
       "      <td>0.116705</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.122118</td>\n",
       "      <td>0.005298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.026231</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_features_to_select': 4}</td>\n",
       "      <td>0.138537</td>\n",
       "      <td>0.132228</td>\n",
       "      <td>0.108690</td>\n",
       "      <td>0.126485</td>\n",
       "      <td>0.012844</td>\n",
       "      <td>36</td>\n",
       "      <td>0.136716</td>\n",
       "      <td>0.137868</td>\n",
       "      <td>0.143218</td>\n",
       "      <td>0.139267</td>\n",
       "      <td>0.002833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.092066</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.007754</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_features_to_select': 5}</td>\n",
       "      <td>0.149230</td>\n",
       "      <td>0.154071</td>\n",
       "      <td>0.115913</td>\n",
       "      <td>0.139738</td>\n",
       "      <td>0.016962</td>\n",
       "      <td>35</td>\n",
       "      <td>0.156090</td>\n",
       "      <td>0.151724</td>\n",
       "      <td>0.151106</td>\n",
       "      <td>0.152973</td>\n",
       "      <td>0.002218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088190</td>\n",
       "      <td>0.004443</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_features_to_select': 6}</td>\n",
       "      <td>0.156724</td>\n",
       "      <td>0.158243</td>\n",
       "      <td>0.130190</td>\n",
       "      <td>0.148386</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>34</td>\n",
       "      <td>0.167222</td>\n",
       "      <td>0.164201</td>\n",
       "      <td>0.161618</td>\n",
       "      <td>0.164347</td>\n",
       "      <td>0.002290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.089537</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>7</td>\n",
       "      <td>{'n_features_to_select': 7}</td>\n",
       "      <td>0.153883</td>\n",
       "      <td>0.161252</td>\n",
       "      <td>0.146581</td>\n",
       "      <td>0.153905</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>33</td>\n",
       "      <td>0.170195</td>\n",
       "      <td>0.166978</td>\n",
       "      <td>0.167139</td>\n",
       "      <td>0.168104</td>\n",
       "      <td>0.001480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.077423</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>8</td>\n",
       "      <td>{'n_features_to_select': 8}</td>\n",
       "      <td>0.152874</td>\n",
       "      <td>0.162557</td>\n",
       "      <td>0.158339</td>\n",
       "      <td>0.157923</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>31</td>\n",
       "      <td>0.175214</td>\n",
       "      <td>0.169502</td>\n",
       "      <td>0.172071</td>\n",
       "      <td>0.172262</td>\n",
       "      <td>0.002336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.071701</td>\n",
       "      <td>0.013015</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>9</td>\n",
       "      <td>{'n_features_to_select': 9}</td>\n",
       "      <td>0.152573</td>\n",
       "      <td>0.162380</td>\n",
       "      <td>0.158027</td>\n",
       "      <td>0.157660</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>32</td>\n",
       "      <td>0.176006</td>\n",
       "      <td>0.170019</td>\n",
       "      <td>0.172644</td>\n",
       "      <td>0.172889</td>\n",
       "      <td>0.002450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.071274</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_features_to_select': 10}</td>\n",
       "      <td>0.156529</td>\n",
       "      <td>0.161022</td>\n",
       "      <td>0.160419</td>\n",
       "      <td>0.159323</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>30</td>\n",
       "      <td>0.176963</td>\n",
       "      <td>0.174870</td>\n",
       "      <td>0.176841</td>\n",
       "      <td>0.176225</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.090612</td>\n",
       "      <td>0.012494</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>11</td>\n",
       "      <td>{'n_features_to_select': 11}</td>\n",
       "      <td>0.156745</td>\n",
       "      <td>0.164493</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.160404</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>29</td>\n",
       "      <td>0.177169</td>\n",
       "      <td>0.177264</td>\n",
       "      <td>0.177047</td>\n",
       "      <td>0.177160</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.075750</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>12</td>\n",
       "      <td>{'n_features_to_select': 12}</td>\n",
       "      <td>0.160838</td>\n",
       "      <td>0.163734</td>\n",
       "      <td>0.159741</td>\n",
       "      <td>0.161438</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>28</td>\n",
       "      <td>0.178855</td>\n",
       "      <td>0.177804</td>\n",
       "      <td>0.177783</td>\n",
       "      <td>0.178148</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.064459</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>13</td>\n",
       "      <td>{'n_features_to_select': 13}</td>\n",
       "      <td>0.163617</td>\n",
       "      <td>0.163375</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>0.162420</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>26</td>\n",
       "      <td>0.179408</td>\n",
       "      <td>0.177930</td>\n",
       "      <td>0.178485</td>\n",
       "      <td>0.178607</td>\n",
       "      <td>0.000610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.068548</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.008599</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>14</td>\n",
       "      <td>{'n_features_to_select': 14}</td>\n",
       "      <td>0.164229</td>\n",
       "      <td>0.162910</td>\n",
       "      <td>0.162315</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2</td>\n",
       "      <td>0.179714</td>\n",
       "      <td>0.178030</td>\n",
       "      <td>0.178603</td>\n",
       "      <td>0.178782</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055925</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_features_to_select': 15}</td>\n",
       "      <td>0.163920</td>\n",
       "      <td>0.163321</td>\n",
       "      <td>0.162123</td>\n",
       "      <td>0.163122</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>3</td>\n",
       "      <td>0.179979</td>\n",
       "      <td>0.178137</td>\n",
       "      <td>0.178670</td>\n",
       "      <td>0.178929</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>16</td>\n",
       "      <td>{'n_features_to_select': 16}</td>\n",
       "      <td>0.162896</td>\n",
       "      <td>0.164077</td>\n",
       "      <td>0.162619</td>\n",
       "      <td>0.163197</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>1</td>\n",
       "      <td>0.180393</td>\n",
       "      <td>0.178251</td>\n",
       "      <td>0.178792</td>\n",
       "      <td>0.179145</td>\n",
       "      <td>0.000909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.040319</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_features_to_select': 17}</td>\n",
       "      <td>0.162881</td>\n",
       "      <td>0.163966</td>\n",
       "      <td>0.162446</td>\n",
       "      <td>0.163098</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>4</td>\n",
       "      <td>0.180485</td>\n",
       "      <td>0.178537</td>\n",
       "      <td>0.178846</td>\n",
       "      <td>0.179289</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.045760</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>18</td>\n",
       "      <td>{'n_features_to_select': 18}</td>\n",
       "      <td>0.163010</td>\n",
       "      <td>0.163466</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.162392</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>27</td>\n",
       "      <td>0.180516</td>\n",
       "      <td>0.178758</td>\n",
       "      <td>0.178985</td>\n",
       "      <td>0.179420</td>\n",
       "      <td>0.000781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.031295</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>19</td>\n",
       "      <td>{'n_features_to_select': 19}</td>\n",
       "      <td>0.163111</td>\n",
       "      <td>0.164302</td>\n",
       "      <td>0.161427</td>\n",
       "      <td>0.162946</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>5</td>\n",
       "      <td>0.180519</td>\n",
       "      <td>0.178887</td>\n",
       "      <td>0.179112</td>\n",
       "      <td>0.179506</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.027306</td>\n",
       "      <td>0.007003</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_features_to_select': 20}</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.164159</td>\n",
       "      <td>0.161484</td>\n",
       "      <td>0.162918</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>6</td>\n",
       "      <td>0.180520</td>\n",
       "      <td>0.178891</td>\n",
       "      <td>0.179126</td>\n",
       "      <td>0.179512</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_features_to_select': 21}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>22</td>\n",
       "      <td>{'n_features_to_select': 22}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.016909</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>23</td>\n",
       "      <td>{'n_features_to_select': 23}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.016308</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>24</td>\n",
       "      <td>{'n_features_to_select': 24}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>25</td>\n",
       "      <td>{'n_features_to_select': 25}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.018092</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>26</td>\n",
       "      <td>{'n_features_to_select': 26}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.015264</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>27</td>\n",
       "      <td>{'n_features_to_select': 27}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>28</td>\n",
       "      <td>{'n_features_to_select': 28}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>29</td>\n",
       "      <td>{'n_features_to_select': 29}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.024279</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_features_to_select': 30}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.015052</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>31</td>\n",
       "      <td>{'n_features_to_select': 31}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>32</td>\n",
       "      <td>{'n_features_to_select': 32}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>33</td>\n",
       "      <td>{'n_features_to_select': 33}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.015990</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>34</td>\n",
       "      <td>{'n_features_to_select': 34}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>35</td>\n",
       "      <td>{'n_features_to_select': 35}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>36</td>\n",
       "      <td>{'n_features_to_select': 36}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.015642</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>37</td>\n",
       "      <td>{'n_features_to_select': 37}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.014404</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>38</td>\n",
       "      <td>{'n_features_to_select': 38}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>39</td>\n",
       "      <td>{'n_features_to_select': 39}</td>\n",
       "      <td>0.163018</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.161487</td>\n",
       "      <td>0.162908</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>7</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.179127</td>\n",
       "      <td>0.179516</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.073271      0.008882         0.004620        0.000471   \n",
       "1        0.069120      0.003712         0.005159        0.002014   \n",
       "2        0.095901      0.026231         0.007135        0.003843   \n",
       "3        0.092066      0.010320         0.007754        0.001927   \n",
       "4        0.088190      0.004443         0.005730        0.001781   \n",
       "5        0.089537      0.009006         0.008486        0.002044   \n",
       "6        0.077423      0.005711         0.006146        0.000203   \n",
       "7        0.071701      0.013015         0.006940        0.000509   \n",
       "8        0.071274      0.012851         0.005912        0.002929   \n",
       "9        0.090612      0.012494         0.009935        0.003481   \n",
       "10       0.075750      0.007368         0.007213        0.001392   \n",
       "11       0.064459      0.008909         0.008521        0.003897   \n",
       "12       0.068548      0.026875         0.008599        0.004864   \n",
       "13       0.055925      0.012426         0.010115        0.004609   \n",
       "14       0.045280      0.006336         0.005770        0.003872   \n",
       "15       0.040319      0.004968         0.006924        0.003249   \n",
       "16       0.045760      0.013788         0.008166        0.001636   \n",
       "17       0.031295      0.001757         0.005211        0.001841   \n",
       "18       0.027306      0.007003         0.008803        0.002125   \n",
       "19       0.022422      0.005393         0.008184        0.001806   \n",
       "20       0.016709      0.002277         0.006323        0.000870   \n",
       "21       0.016909      0.003548         0.007389        0.000256   \n",
       "22       0.016308      0.001910         0.005158        0.001838   \n",
       "23       0.023928      0.006085         0.009588        0.004290   \n",
       "24       0.018092      0.005178         0.009774        0.003177   \n",
       "25       0.015264      0.000972         0.006658        0.001024   \n",
       "26       0.015876      0.001950         0.006183        0.001853   \n",
       "27       0.016427      0.001840         0.006999        0.000465   \n",
       "28       0.024279      0.006818         0.010159        0.003033   \n",
       "29       0.015052      0.000879         0.007036        0.000453   \n",
       "30       0.014684      0.000104         0.006935        0.000522   \n",
       "31       0.013721      0.000911         0.006024        0.002197   \n",
       "32       0.015990      0.001818         0.004608        0.001916   \n",
       "33       0.014821      0.000374         0.007367        0.000098   \n",
       "34       0.015974      0.002086         0.007330        0.000161   \n",
       "35       0.015642      0.000796         0.006504        0.000897   \n",
       "36       0.014404      0.000059         0.007663        0.000131   \n",
       "37       0.014936      0.000615         0.006611        0.000585   \n",
       "\n",
       "   param_n_features_to_select                        params  \\\n",
       "0                           2   {'n_features_to_select': 2}   \n",
       "1                           3   {'n_features_to_select': 3}   \n",
       "2                           4   {'n_features_to_select': 4}   \n",
       "3                           5   {'n_features_to_select': 5}   \n",
       "4                           6   {'n_features_to_select': 6}   \n",
       "5                           7   {'n_features_to_select': 7}   \n",
       "6                           8   {'n_features_to_select': 8}   \n",
       "7                           9   {'n_features_to_select': 9}   \n",
       "8                          10  {'n_features_to_select': 10}   \n",
       "9                          11  {'n_features_to_select': 11}   \n",
       "10                         12  {'n_features_to_select': 12}   \n",
       "11                         13  {'n_features_to_select': 13}   \n",
       "12                         14  {'n_features_to_select': 14}   \n",
       "13                         15  {'n_features_to_select': 15}   \n",
       "14                         16  {'n_features_to_select': 16}   \n",
       "15                         17  {'n_features_to_select': 17}   \n",
       "16                         18  {'n_features_to_select': 18}   \n",
       "17                         19  {'n_features_to_select': 19}   \n",
       "18                         20  {'n_features_to_select': 20}   \n",
       "19                         21  {'n_features_to_select': 21}   \n",
       "20                         22  {'n_features_to_select': 22}   \n",
       "21                         23  {'n_features_to_select': 23}   \n",
       "22                         24  {'n_features_to_select': 24}   \n",
       "23                         25  {'n_features_to_select': 25}   \n",
       "24                         26  {'n_features_to_select': 26}   \n",
       "25                         27  {'n_features_to_select': 27}   \n",
       "26                         28  {'n_features_to_select': 28}   \n",
       "27                         29  {'n_features_to_select': 29}   \n",
       "28                         30  {'n_features_to_select': 30}   \n",
       "29                         31  {'n_features_to_select': 31}   \n",
       "30                         32  {'n_features_to_select': 32}   \n",
       "31                         33  {'n_features_to_select': 33}   \n",
       "32                         34  {'n_features_to_select': 34}   \n",
       "33                         35  {'n_features_to_select': 35}   \n",
       "34                         36  {'n_features_to_select': 36}   \n",
       "35                         37  {'n_features_to_select': 37}   \n",
       "36                         38  {'n_features_to_select': 38}   \n",
       "37                         39  {'n_features_to_select': 39}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0            0.082294           0.102248           0.071757         0.085433   \n",
       "1            0.122841           0.128139           0.104549         0.118510   \n",
       "2            0.138537           0.132228           0.108690         0.126485   \n",
       "3            0.149230           0.154071           0.115913         0.139738   \n",
       "4            0.156724           0.158243           0.130190         0.148386   \n",
       "5            0.153883           0.161252           0.146581         0.153905   \n",
       "6            0.152874           0.162557           0.158339         0.157923   \n",
       "7            0.152573           0.162380           0.158027         0.157660   \n",
       "8            0.156529           0.161022           0.160419         0.159323   \n",
       "9            0.156745           0.164493           0.159973         0.160404   \n",
       "10           0.160838           0.163734           0.159741         0.161438   \n",
       "11           0.163617           0.163375           0.160269         0.162420   \n",
       "12           0.164229           0.162910           0.162315         0.163151   \n",
       "13           0.163920           0.163321           0.162123         0.163122   \n",
       "14           0.162896           0.164077           0.162619         0.163197   \n",
       "15           0.162881           0.163966           0.162446         0.163098   \n",
       "16           0.163010           0.163466           0.160700         0.162392   \n",
       "17           0.163111           0.164302           0.161427         0.162946   \n",
       "18           0.163112           0.164159           0.161484         0.162918   \n",
       "19           0.163018           0.164220           0.161487         0.162908   \n",
       "20           0.163018           0.164220           0.161487         0.162908   \n",
       "21           0.163018           0.164220           0.161487         0.162908   \n",
       "22           0.163018           0.164220           0.161487         0.162908   \n",
       "23           0.163018           0.164220           0.161487         0.162908   \n",
       "24           0.163018           0.164220           0.161487         0.162908   \n",
       "25           0.163018           0.164220           0.161487         0.162908   \n",
       "26           0.163018           0.164220           0.161487         0.162908   \n",
       "27           0.163018           0.164220           0.161487         0.162908   \n",
       "28           0.163018           0.164220           0.161487         0.162908   \n",
       "29           0.163018           0.164220           0.161487         0.162908   \n",
       "30           0.163018           0.164220           0.161487         0.162908   \n",
       "31           0.163018           0.164220           0.161487         0.162908   \n",
       "32           0.163018           0.164220           0.161487         0.162908   \n",
       "33           0.163018           0.164220           0.161487         0.162908   \n",
       "34           0.163018           0.164220           0.161487         0.162908   \n",
       "35           0.163018           0.164220           0.161487         0.162908   \n",
       "36           0.163018           0.164220           0.161487         0.162908   \n",
       "37           0.163018           0.164220           0.161487         0.162908   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.012644               38            0.097624            0.085776   \n",
       "1         0.010106               37            0.120339            0.116705   \n",
       "2         0.012844               36            0.136716            0.137868   \n",
       "3         0.016962               35            0.156090            0.151724   \n",
       "4         0.012882               34            0.167222            0.164201   \n",
       "5         0.005990               33            0.170195            0.166978   \n",
       "6         0.003964               31            0.175214            0.169502   \n",
       "7         0.004012               32            0.176006            0.170019   \n",
       "8         0.001991               30            0.176963            0.174870   \n",
       "9         0.003178               29            0.177169            0.177264   \n",
       "10        0.001684               28            0.178855            0.177804   \n",
       "11        0.001524               26            0.179408            0.177930   \n",
       "12        0.000800                2            0.179714            0.178030   \n",
       "13        0.000747                3            0.179979            0.178137   \n",
       "14        0.000632                1            0.180393            0.178251   \n",
       "15        0.000639                4            0.180485            0.178537   \n",
       "16        0.001211               27            0.180516            0.178758   \n",
       "17        0.001179                5            0.180519            0.178887   \n",
       "18        0.001101                6            0.180520            0.178891   \n",
       "19        0.001118                7            0.180521            0.178900   \n",
       "20        0.001118                7            0.180521            0.178900   \n",
       "21        0.001118                7            0.180521            0.178900   \n",
       "22        0.001118                7            0.180521            0.178900   \n",
       "23        0.001118                7            0.180521            0.178900   \n",
       "24        0.001118                7            0.180521            0.178900   \n",
       "25        0.001118                7            0.180521            0.178900   \n",
       "26        0.001118                7            0.180521            0.178900   \n",
       "27        0.001118                7            0.180521            0.178900   \n",
       "28        0.001118                7            0.180521            0.178900   \n",
       "29        0.001118                7            0.180521            0.178900   \n",
       "30        0.001118                7            0.180521            0.178900   \n",
       "31        0.001118                7            0.180521            0.178900   \n",
       "32        0.001118                7            0.180521            0.178900   \n",
       "33        0.001118                7            0.180521            0.178900   \n",
       "34        0.001118                7            0.180521            0.178900   \n",
       "35        0.001118                7            0.180521            0.178900   \n",
       "36        0.001118                7            0.180521            0.178900   \n",
       "37        0.001118                7            0.180521            0.178900   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "0             0.102462          0.095287         0.007009  \n",
       "1             0.129310          0.122118         0.005298  \n",
       "2             0.143218          0.139267         0.002833  \n",
       "3             0.151106          0.152973         0.002218  \n",
       "4             0.161618          0.164347         0.002290  \n",
       "5             0.167139          0.168104         0.001480  \n",
       "6             0.172071          0.172262         0.002336  \n",
       "7             0.172644          0.172889         0.002450  \n",
       "8             0.176841          0.176225         0.000959  \n",
       "9             0.177047          0.177160         0.000088  \n",
       "10            0.177783          0.178148         0.000501  \n",
       "11            0.178485          0.178607         0.000610  \n",
       "12            0.178603          0.178782         0.000699  \n",
       "13            0.178670          0.178929         0.000774  \n",
       "14            0.178792          0.179145         0.000909  \n",
       "15            0.178846          0.179289         0.000855  \n",
       "16            0.178985          0.179420         0.000781  \n",
       "17            0.179112          0.179506         0.000723  \n",
       "18            0.179126          0.179512         0.000719  \n",
       "19            0.179127          0.179516         0.000717  \n",
       "20            0.179127          0.179516         0.000717  \n",
       "21            0.179127          0.179516         0.000717  \n",
       "22            0.179127          0.179516         0.000717  \n",
       "23            0.179127          0.179516         0.000717  \n",
       "24            0.179127          0.179516         0.000717  \n",
       "25            0.179127          0.179516         0.000717  \n",
       "26            0.179127          0.179516         0.000717  \n",
       "27            0.179127          0.179516         0.000717  \n",
       "28            0.179127          0.179516         0.000717  \n",
       "29            0.179127          0.179516         0.000717  \n",
       "30            0.179127          0.179516         0.000717  \n",
       "31            0.179127          0.179516         0.000717  \n",
       "32            0.179127          0.179516         0.000717  \n",
       "33            0.179127          0.179516         0.000717  \n",
       "34            0.179127          0.179516         0.000717  \n",
       "35            0.179127          0.179516         0.000717  \n",
       "36            0.179127          0.179516         0.000717  \n",
       "37            0.179127          0.179516         0.000717  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20befa353d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1144699672004246"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "error = mean_squared_error(Y_test, predictions)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13983943993318448"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsquar = r2_score(Y_test, predictions)\n",
    "Rsquar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0343425428939734"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_train = mean_squared_error(Y_train, predictions_train)\n",
    "error_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree  (using Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'presort': 'deprecated',\n",
      " 'random_state': 0,\n",
      " 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DTR = DecisionTreeRegressor(random_state=0)\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(DTR.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR.fit(X_train, Y_train)\n",
    "Y_pred = DTR.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test,Y_pred)\n",
    "r_squared = r2_score (Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree (using 3 cross validation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [0,\n",
      "               5,\n",
      "               10,\n",
      "               15,\n",
      "               20,\n",
      "               25,\n",
      "               30,\n",
      "               35,\n",
      "               41,\n",
      "               46,\n",
      "               51,\n",
      "               56,\n",
      "               61,\n",
      "               66,\n",
      "               71,\n",
      "               76,\n",
      "               82,\n",
      "               87,\n",
      "               92,\n",
      "               97,\n",
      "               102,\n",
      "               107,\n",
      "               112,\n",
      "               117,\n",
      "               123,\n",
      "               128,\n",
      "               133,\n",
      "               138,\n",
      "               143,\n",
      "               148,\n",
      "               153,\n",
      "               158,\n",
      "               164,\n",
      "               169,\n",
      "               174,\n",
      "               179,\n",
      "               184,\n",
      "               189,\n",
      "               194,\n",
      "               200,\n",
      "               None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(0, 200, num = 40)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf\n",
    "               }\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 127 out of 150 | elapsed:    5.7s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=DecisionTreeRegressor(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [0, 5, 10, 15, 20, 25, 30,\n",
       "                                                      35, 41, 46, 51, 56, 61,\n",
       "                                                      66, 71, 76, 82, 87, 92,\n",
       "                                                      97, 102, 107, 112, 117,\n",
       "                                                      123, 128, 133, 138, 143,\n",
       "                                                      148, ...],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "regressor = DecisionTreeRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train3, Y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 2,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 5}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = DecisionTreeRegressor(min_samples_split=2,min_samples_leaf=4,max_features=\"sqrt\",max_depth=5,random_state=0).fit(X_train, Y_train)\n",
    "Y_pred2 = reg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse2 = mean_squared_error(Y_test, Y_pred2)\n",
    "rmse2 = sqrt(mse2)\n",
    "mae2 = mean_absolute_error(Y_test,Y_pred2)\n",
    "r_squared2 = r2_score (Y_test,Y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'MSE': [mse, mse2],\n",
    "        'RMSE': [rmse, rmse2],\n",
    "        'MAE': [mae, mae2],\n",
    "        'R': [r_squared, r_squared2]}\n",
    "dtrd = pd.DataFrame(data,columns=['MSE', 'RMSE','MAE','R'],index=['with default hyper parameter', 'with 3-cross valdiation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>with default hyper parameter</th>\n",
       "      <td>1.583895</td>\n",
       "      <td>1.258529</td>\n",
       "      <td>0.871806</td>\n",
       "      <td>-0.222468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with 3-cross valdiation</th>\n",
       "      <td>1.122785</td>\n",
       "      <td>1.059616</td>\n",
       "      <td>0.821678</td>\n",
       "      <td>0.133422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   MSE      RMSE       MAE        R\n",
       "with default hyper parameter  1.583895  1.258529  0.871806 -0.222468\n",
       "with 3-cross valdiation       1.122785  1.059616  0.821678  0.133422"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.345     , 2.387     , 3.4304784 , 1.861     , 2.0985    ,\n",
       "       1.93574272, 2.2085    , 1.8035    , 2.95901611, 2.36909863,\n",
       "       2.12488911, 2.28295564, 2.82492859, 1.901     , 2.136     ,\n",
       "       4.61812662, 2.385     , 3.49571551, 1.6605    , 2.17375   ,\n",
       "       2.53467183, 2.19695707, 1.5985    , 2.289     , 2.0915    ,\n",
       "       1.554     , 3.01052506, 2.4135    , 2.369     , 1.8275    ,\n",
       "       3.15904863, 2.68606041, 3.14093208, 3.61612184, 2.62675243,\n",
       "       2.23504492, 2.9259038 , 1.346     , 2.6415    , 2.40279125,\n",
       "       2.018     , 3.13354465, 1.6675    , 2.077     , 2.33672428,\n",
       "       4.22701674, 2.83676691, 2.87081227, 1.5095    , 2.05443044,\n",
       "       2.43088586, 1.6705    , 1.7395    , 2.37350347, 1.482     ,\n",
       "       1.5755    , 4.01233187, 1.554     , 1.5485    , 2.0575    ,\n",
       "       4.94653979, 1.418     , 2.78609516, 2.113     , 3.85364041,\n",
       "       2.01978246, 1.6705    , 1.6335    , 1.9705    , 1.679     ,\n",
       "       1.86721021, 3.02069967, 2.5375    , 2.29704637, 2.68439435,\n",
       "       1.8035    , 2.029     , 2.0125    , 1.984     , 2.107     ,\n",
       "       3.23907152, 3.36336923, 2.3975    , 2.8475    , 2.50977067,\n",
       "       2.62842856, 3.26163762, 2.60119547, 2.046     , 2.4705866 ,\n",
       "       1.8735    , 3.12087636, 3.03698273, 2.001     , 2.76889335,\n",
       "       3.30125   , 2.1325    , 2.322     , 2.95075023, 2.612     ,\n",
       "       2.8035    , 2.17932391, 1.7785    , 2.8008772 , 1.90274272,\n",
       "       3.02071678, 1.70124272, 2.3875    , 2.943     , 2.574     ,\n",
       "       2.7250199 , 2.6380199 , 2.9392334 , 2.79129551, 1.981     ,\n",
       "       2.0225    , 2.61414612, 1.77038911, 2.72756932, 1.80064116,\n",
       "       2.0955    , 2.6593897 , 3.11119688, 3.0241103 , 2.44572933,\n",
       "       2.4665    , 2.22816682, 3.34699641, 1.6885    , 2.51793044,\n",
       "       3.58586538, 2.065     , 2.9195    , 2.4035    , 2.33703743,\n",
       "       1.441     , 1.94      , 3.37769439, 2.36882174, 2.21825243,\n",
       "       2.23094721, 1.946     , 3.07205575, 2.5675    , 1.712     ,\n",
       "       2.11216682, 1.85840978, 2.036     , 1.8135    , 2.4897301 ,\n",
       "       1.762     , 1.321     , 2.72396159, 2.8285    , 2.14464216,\n",
       "       1.576     , 3.60368287, 2.3225    , 3.19990416, 1.406     ,\n",
       "       2.0535    , 2.035     , 2.95753828, 2.78273134, 2.6340475 ,\n",
       "       1.758     , 2.753     , 2.37359578, 1.641     , 3.17009637,\n",
       "       2.5175    , 2.6400693 , 3.49545646, 1.671     , 1.4495    ,\n",
       "       2.7379206 , 2.623     , 2.75621731, 2.03522992, 2.5322356 ,\n",
       "       2.1       , 3.64751517, 1.407     , 1.733     , 2.72917744,\n",
       "       3.19576027, 2.73753232, 3.98812662, 3.13882374, 2.85538612,\n",
       "       2.0998805 , 3.16538967, 1.97624757, 1.6765    , 2.5005    ,\n",
       "       2.54825439, 2.022     , 2.3385    , 1.489     , 1.487     ,\n",
       "       2.461     , 3.59886294, 1.9       , 2.62013158, 1.272     ,\n",
       "       2.1149072 , 1.9295    , 1.829     , 2.16563419, 2.3285    ,\n",
       "       2.521     , 3.60435074, 2.28805791, 2.00037287, 2.83245306,\n",
       "       2.2035    , 2.0395    , 1.542     , 3.06490422, 2.0835    ,\n",
       "       2.49710143, 1.52874272, 2.51761481, 3.03356285, 2.08866667,\n",
       "       1.703     , 3.30506332, 2.2575    , 2.53946807, 1.623     ,\n",
       "       2.3775    , 1.648     , 1.659     , 1.852     , 2.79622779,\n",
       "       1.2435    , 1.6915    , 2.46309516, 1.8905    , 2.86907134,\n",
       "       1.678     , 3.73744122, 4.67836824, 2.45575243, 1.91226864,\n",
       "       4.90756253, 2.90409741, 2.42404093, 1.5085    , 2.71      ,\n",
       "       1.546     , 3.1436692 , 2.54965396, 3.70582391, 1.91204632,\n",
       "       3.0128199 , 1.929     , 1.5855    , 4.12763396, 2.15      ,\n",
       "       3.76460146, 1.5465    , 2.6875    , 2.45      , 1.5135    ,\n",
       "       1.77      , 3.6555058 , 2.45094806, 2.91995454, 2.87343901,\n",
       "       1.677     , 1.456     , 4.218     , 2.2125    , 2.8611722 ,\n",
       "       1.84315602, 3.02973619, 1.779     , 1.532     , 2.16380593,\n",
       "       1.311     , 2.52676864, 2.822     , 2.6875    , 2.399     ,\n",
       "       2.0385    , 2.0615    , 2.36705913, 2.06787338, 3.1033537 ,\n",
       "       2.1795    , 2.13      , 2.65325   , 2.98681634, 1.4075    ,\n",
       "       2.17569443, 1.978     , 2.1876856 , 2.7935494 , 1.576     ,\n",
       "       3.01996814, 2.56776825, 3.6827987 , 2.89238911, 3.09806285,\n",
       "       2.98813486, 2.57560822, 2.2575    , 1.5085    , 2.59315197,\n",
       "       2.53261719, 2.5455    , 4.27678441, 2.0865    , 1.73474272,\n",
       "       1.5135    , 1.5935    , 2.216     , 3.64661581, 2.428     ,\n",
       "       1.844     , 3.32632391, 3.33136581, 2.10828601, 1.9145    ,\n",
       "       2.245     , 2.44513619, 1.9555    , 2.1895    , 2.51827067,\n",
       "       2.547     , 3.56095327, 1.90328601, 2.67009741, 2.311     ,\n",
       "       1.5775    , 3.11925581, 2.1505    , 2.378312  , 1.492     ,\n",
       "       2.0855    , 2.89022992, 2.1665    , 2.831     , 2.98523372,\n",
       "       2.67      , 1.975     , 2.58516854, 3.063     , 1.846     ,\n",
       "       1.868     , 2.25444721, 1.478     , 1.6075    , 2.56559217,\n",
       "       2.15746122, 1.92635388, 1.482     , 1.96328601, 2.073     ,\n",
       "       2.84854093, 2.1105    , 2.73069066, 2.72444806, 2.014     ,\n",
       "       1.8975    , 2.416     , 2.22674757, 4.14895914, 2.0785    ,\n",
       "       1.8035    , 1.635     , 2.51593976, 3.09556032, 2.233     ,\n",
       "       2.201     , 3.04620499, 2.65694868, 2.15605791, 1.98063419,\n",
       "       2.50030056, 1.90267856, 1.3675    , 2.30034151, 1.90163586,\n",
       "       1.836     , 3.36413211, 3.0905    , 2.14675   , 2.1075    ,\n",
       "       2.061     , 1.5475    , 1.8455    , 1.332     , 2.44321371,\n",
       "       1.6275    , 2.71693044, 2.72968964, 2.65068138, 1.99675728,\n",
       "       2.55025   , 2.6345    , 1.9285    , 4.78703822, 1.70171021,\n",
       "       2.9326068 , 2.60675333, 3.0535596 , 1.564     , 1.7635    ,\n",
       "       2.41075   , 2.78075   , 1.8535    , 2.2322356 , 2.34051203,\n",
       "       1.4485    , 2.067     , 1.9       , 2.34197557, 3.02157397,\n",
       "       3.1793805 , 2.6475    , 3.03554493, 2.31673606, 2.54301456,\n",
       "       2.85147227, 2.43487338, 2.0695    , 3.014     , 2.99196176,\n",
       "       1.7115    , 2.42225   , 1.578     , 2.2425    , 4.0931492 ,\n",
       "       2.68936824, 1.89856932, 2.46548746, 1.4655    , 2.1585    ,\n",
       "       2.015     , 3.04575324, 2.392     , 2.134     , 1.565     ,\n",
       "       1.8245    , 3.03261591, 4.75087353, 1.7545    , 1.614     ,\n",
       "       2.12832228, 2.68811824, 2.4603603 , 2.81884153, 2.72966009,\n",
       "       1.8605    , 2.987     , 2.6855    , 1.84416667, 2.0425    ,\n",
       "       1.717     , 2.46394721, 2.03459217, 1.763     , 2.0585    ,\n",
       "       1.261     , 4.63310784, 2.5179986 , 2.315     , 1.987     ,\n",
       "       2.23290422, 2.70345111, 2.20776864, 2.107     , 2.04798871,\n",
       "       3.36314154, 3.35026927, 1.7215    , 2.88577008, 3.14341919,\n",
       "       1.5615    , 3.43166442, 2.32675   , 1.54545564, 2.1665    ,\n",
       "       2.53774272, 2.186     , 4.103     , 1.93093044, 3.48900055,\n",
       "       1.671     , 2.118     , 1.375     , 2.1755    , 1.752     ,\n",
       "       3.6925182 , 2.283     , 1.705     , 1.982     , 1.9305    ,\n",
       "       2.6165    , 3.09831882, 3.40513804, 2.87861362, 1.4915    ,\n",
       "       2.364     , 2.012     , 3.13222933, 2.374     , 2.88140512,\n",
       "       2.39074597, 1.71552067, 1.815     , 2.0675    , 2.86697126,\n",
       "       2.60713214, 2.021     , 1.99771021, 1.5465    , 1.966     ,\n",
       "       1.392     , 1.299     , 2.71150069, 1.99804492, 1.268     ,\n",
       "       1.7395    , 2.99792721, 2.055     , 2.57057842, 1.8655    ,\n",
       "       2.3975    , 2.48128655, 3.36992628, 3.93657391, 2.89869761,\n",
       "       1.8175    , 1.55506024, 2.337     , 1.9005    , 3.6136875 ,\n",
       "       2.71043044, 2.2835    , 3.13728236, 1.279     , 2.96036604,\n",
       "       1.4715    , 2.68185065, 2.65122883, 2.70175811, 3.01552655,\n",
       "       2.5285    , 2.73163419, 2.931     , 3.23448716, 3.06073606,\n",
       "       2.62964772, 1.458     , 1.79074272, 2.076     , 2.7984457 ,\n",
       "       2.89123861, 2.66807457, 3.1366651 , 2.0605    , 2.3685    ,\n",
       "       2.80300347, 2.37672883, 2.2255    , 1.722     , 1.534     ,\n",
       "       2.017     , 3.4411247 , 2.78475   , 1.863     , 2.3525    ,\n",
       "       2.41510875, 2.50659132, 2.2985    , 2.65425   , 2.4455    ,\n",
       "       1.5345    , 3.0005    , 2.1425    , 3.06097227, 2.66113863,\n",
       "       1.467     , 2.87229086, 1.6605    , 2.1255    , 3.29037689,\n",
       "       1.6035    , 3.0393704 , 2.3502482 , 3.57211734, 1.6175    ,\n",
       "       2.43790512, 2.87540594, 2.55591518, 2.26654473, 2.27559741,\n",
       "       2.3765    , 2.63882492, 3.50161824, 1.988     , 2.24614116,\n",
       "       2.68629711, 1.8014801 , 2.42388911, 1.57685388, 1.7065    ,\n",
       "       1.651     , 4.52622403, 1.71      , 2.14      , 1.7165    ,\n",
       "       1.6975    , 1.84675   , 2.5075    , 2.6125    , 2.34949514,\n",
       "       3.25038911, 2.33528216, 3.42960071, 1.817     , 2.1685    ,\n",
       "       2.46242744, 2.231     , 1.7205    , 2.30320975, 2.18331494,\n",
       "       2.41922992, 2.10713586, 1.99764116, 1.9275    , 3.1905    ,\n",
       "       1.587     , 1.4155    , 1.482     , 1.585     , 2.4525    ,\n",
       "       1.946     , 2.68743741, 2.50778655, 2.63259741, 2.44205791,\n",
       "       2.076     , 1.314     , 3.04821618, 2.1785    , 2.6433266 ,\n",
       "       1.929     , 1.687     , 1.8805    , 1.9005    , 2.74753244,\n",
       "       2.4865    , 1.817     , 2.72853878, 2.21125   , 3.70076425,\n",
       "       3.68300786, 3.58183739, 3.11561147, 2.002     , 1.788     ,\n",
       "       2.95940978, 2.1095    , 2.42617856, 1.4735    , 2.106     ,\n",
       "       2.104     , 1.43      , 2.02693044, 2.6675    , 3.30461214,\n",
       "       2.60150442, 3.26765677, 1.5155    , 2.098     , 2.7555    ,\n",
       "       1.8065    , 2.16371618, 2.20895564, 2.48063419, 2.3695    ,\n",
       "       2.31321171, 2.49872992, 2.13901394, 1.592     , 1.5205    ,\n",
       "       3.29808373, 1.4685    , 1.754     , 2.75872933, 3.42181501,\n",
       "       1.3665    , 1.562     , 1.637     , 2.72884322, 2.28966682,\n",
       "       2.027     , 1.9175    , 1.752     , 2.816     , 2.873     ,\n",
       "       2.72978635, 2.3205    , 1.71171021, 1.782     , 2.441     ,\n",
       "       2.92906956, 1.7675    , 3.66517094, 2.82878628, 1.9115    ,\n",
       "       3.25351945, 1.732     , 1.6405    , 1.7455    , 2.407     ,\n",
       "       2.28543068, 1.7825    , 2.00544863, 2.44572428, 2.29109516,\n",
       "       2.31375   , 1.7365    , 1.61759516, 1.5265    , 2.06480593,\n",
       "       2.88416989, 2.8055    , 2.48      , 2.3855    , 3.25681579,\n",
       "       2.60456956, 2.0025    , 2.4742908 , 2.49534322, 2.54555791,\n",
       "       2.02736824, 3.34104086, 2.71386301, 1.2485    , 3.56675028,\n",
       "       3.1272911 , 1.941     , 2.106     , 2.11484322, 1.73574272,\n",
       "       3.31991813, 1.704     , 2.586     , 2.5235    , 2.79038082,\n",
       "       1.5545    , 1.99654473, 2.5565    , 2.78600995, 2.39804492,\n",
       "       1.4935    , 1.7325    , 3.09308187, 3.541     , 2.171     ,\n",
       "       2.7405    , 1.576     , 1.92966706, 2.0935    , 2.45954432,\n",
       "       2.2005    , 2.664     , 2.1935    , 3.08876017, 1.789     ,\n",
       "       2.76121627, 2.82632628, 2.98847727, 2.10509578, 3.83125864,\n",
       "       2.02427957, 2.2186195 , 1.358     , 1.554     , 2.3915    ,\n",
       "       2.48264528, 3.28702085, 1.62      , 1.9375    , 2.1965    ,\n",
       "       1.8375    , 2.188     , 2.38721021, 2.1805    , 2.344     ,\n",
       "       3.10860672, 2.1055    , 2.28092837, 1.96578012, 2.6095    ,\n",
       "       2.31214116, 2.46122933, 2.66988307, 2.4575    , 2.64675324,\n",
       "       2.93248437, 2.072     , 2.44275   , 2.65001215, 1.4735    ,\n",
       "       2.0495    , 2.8028897 , 2.1075    , 1.452     , 2.097     ,\n",
       "       1.44      , 3.352     , 1.75624757, 2.13174716, 2.8435    ])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9145924936853504"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error3 = mean_squared_error(Y_test, Y_pred)\n",
    "error3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.735814532768672\n",
      "Mean Squared Error: 0.9145924936853504\n",
      "Root Mean Squared Error: 0.9563432928009431\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29410714083440326"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsquar = r2_score(Y_test, Y_pred)\n",
    "Rsquar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6030677 , 2.91480971, 3.27495273, 1.91245521, 2.43404764,\n",
       "       1.81101499, 2.08973681, 1.90637246, 2.67419943, 2.43359782,\n",
       "       2.25759894, 1.95667901, 2.79139466, 1.99751107, 2.0441102 ,\n",
       "       4.11813417, 2.11002161, 2.90923651, 1.5082876 , 2.42491665,\n",
       "       2.44659532, 2.4023884 , 1.71949399, 2.69690701, 2.2398792 ,\n",
       "       1.98439901, 2.36141868, 2.2937356 , 2.68214332, 1.65190038,\n",
       "       2.95240558, 3.25668915, 2.85806121, 2.4750101 , 2.3371302 ,\n",
       "       2.20714322, 2.62628451, 1.62097949, 2.60448399, 2.54331439,\n",
       "       2.29844699, 2.50177209, 1.77161725, 2.45747919, 2.10664396,\n",
       "       4.80783228, 3.0373022 , 2.49473928, 1.69868895, 2.10617705,\n",
       "       2.67101681, 1.62757156, 1.89152563, 2.69646334, 1.29186912,\n",
       "       1.61375267, 3.36537916, 1.32531455, 1.52748901, 2.06155429,\n",
       "       4.79016018, 1.68945479, 2.04959671, 2.26817886, 3.54380325,\n",
       "       1.92224172, 1.71656575, 1.98588989, 2.50263347, 1.76351903,\n",
       "       2.0865771 , 2.62894933, 2.44289588, 1.96660115, 2.36667268,\n",
       "       1.87269194, 1.76253202, 2.09142157, 2.14242975, 2.29617987,\n",
       "       2.94104226, 2.97269734, 2.03612738, 2.92903221, 2.05459487,\n",
       "       2.35802363, 2.726027  , 2.53727789, 1.98355252, 2.64378891,\n",
       "       2.19139707, 2.95324475, 3.15884432, 1.88291394, 2.97959627,\n",
       "       3.12607062, 2.1928975 , 2.37474936, 2.82939975, 2.47405079,\n",
       "       2.43470717, 2.17796432, 1.7680027 , 2.41068393, 1.68709519,\n",
       "       3.00259176, 1.73131618, 2.53581354, 2.97580878, 2.33237344,\n",
       "       2.7975247 , 2.58455065, 2.58822358, 2.94699395, 2.14834786,\n",
       "       1.79022964, 2.68357668, 2.46766565, 2.40691184, 1.80279108,\n",
       "       2.20731124, 2.16436825, 3.02895533, 2.86715108, 2.21229562,\n",
       "       2.07454151, 1.7152668 , 2.80622336, 2.17856871, 2.24793773,\n",
       "       2.8640295 , 2.12467378, 2.22038413, 1.81668047, 2.25032176,\n",
       "       1.62064664, 2.10932362, 2.29507278, 2.22998442, 2.19089059,\n",
       "       1.45540254, 2.26329354, 2.49964088, 2.19561898, 1.93069182,\n",
       "       1.92507109, 1.83004348, 1.7539708 , 1.83447809, 2.31228279,\n",
       "       1.48840585, 1.35199237, 2.32674402, 2.81473217, 2.58949276,\n",
       "       1.61898773, 3.88731006, 2.39924485, 3.25751013, 1.67222017,\n",
       "       1.34039468, 1.65162321, 2.76007659, 2.38567504, 2.76046107,\n",
       "       1.69128172, 2.41388296, 2.79677001, 1.97194791, 3.3718564 ,\n",
       "       2.36134339, 2.67771119, 2.72783848, 1.71587832, 1.59916861,\n",
       "       3.05496729, 3.11778172, 2.78758671, 2.26393209, 2.34667647,\n",
       "       2.87252446, 3.17756311, 1.6003535 , 2.29780845, 2.12452064,\n",
       "       3.6853778 , 2.42273762, 3.54320373, 2.82071512, 2.20744085,\n",
       "       2.18885049, 2.40711216, 2.03023911, 1.65880509, 2.53527396,\n",
       "       2.67363807, 2.13537267, 2.41471076, 1.81490137, 1.73016875,\n",
       "       2.55904897, 4.06099624, 2.11912576, 2.4978791 , 1.46749177,\n",
       "       2.44071554, 1.67632585, 1.96629069, 2.22836828, 2.51992326,\n",
       "       2.53139917, 3.03229253, 2.39301592, 1.77296868, 2.3459019 ,\n",
       "       2.1195262 , 2.22158484, 1.59522144, 3.12533129, 2.11397652,\n",
       "       1.95140186, 1.75682144, 2.76868664, 2.74468362, 1.93571982,\n",
       "       1.70313092, 2.73696981, 2.46527842, 2.721603  , 1.74639441,\n",
       "       2.21998475, 1.74165902, 1.93483219, 2.16664849, 2.59715149,\n",
       "       1.50374134, 1.96890005, 1.75713258, 2.13708267, 2.36326838,\n",
       "       2.12449793, 3.04470697, 4.65740733, 2.22278059, 2.10456886,\n",
       "       4.18856309, 2.75138242, 2.13594219, 2.12821886, 2.99110611,\n",
       "       1.43172722, 3.01949386, 2.46866716, 3.55731718, 2.41373131,\n",
       "       2.61967901, 1.86260932, 1.77040117, 4.34495839, 2.32851064,\n",
       "       2.98019798, 1.69994817, 2.25831682, 2.29228534, 1.94522959,\n",
       "       1.97124521, 3.18203743, 2.15858435, 3.06850676, 2.47045084,\n",
       "       1.93562486, 1.61958485, 4.2332949 , 2.53402184, 2.96155231,\n",
       "       1.81295979, 3.90170893, 2.45456023, 1.68932576, 2.31927058,\n",
       "       1.46409735, 2.17798737, 2.3707049 , 2.06954983, 2.58306946,\n",
       "       2.24262517, 2.11316076, 2.63298772, 2.07828528, 2.47141509,\n",
       "       2.70657151, 2.9117372 , 2.40696814, 2.36666726, 1.413921  ,\n",
       "       2.21110717, 2.14576435, 2.61073736, 2.54078241, 1.60334599,\n",
       "       2.3351166 , 2.03406905, 3.3270284 , 2.37124206, 2.65528475,\n",
       "       2.52517711, 2.62000684, 2.25554491, 1.54257242, 2.74068439,\n",
       "       2.63806558, 2.75078735, 4.18247367, 2.23218687, 1.73944961,\n",
       "       1.28946843, 1.858608  , 2.31893489, 3.33358259, 2.39347471,\n",
       "       2.23108145, 2.7047    , 2.54008955, 1.95813988, 2.05310876,\n",
       "       2.12815933, 2.58553969, 2.64020807, 1.74123363, 2.39342775,\n",
       "       2.31181685, 3.68336129, 1.68911111, 2.66324666, 2.01882499,\n",
       "       1.8513272 , 2.91023754, 2.68048694, 2.0755925 , 1.52957004,\n",
       "       2.08330097, 2.67157484, 2.38545481, 2.67679037, 2.60931572,\n",
       "       2.266516  , 2.04517674, 2.41357802, 2.9568466 , 2.11526655,\n",
       "       2.23180353, 1.83808676, 1.3413951 , 1.67705377, 2.53510944,\n",
       "       2.55496665, 1.97568052, 1.68555671, 1.81129018, 2.46081961,\n",
       "       1.967176  , 1.91252122, 2.13985065, 2.30295985, 1.88071894,\n",
       "       2.6449836 , 2.42898525, 2.18002782, 3.58321907, 1.9363828 ,\n",
       "       1.69927379, 2.14260959, 2.4791424 , 2.20367244, 3.22712448,\n",
       "       2.1268529 , 2.99238642, 2.34613281, 2.62347716, 2.06192295,\n",
       "       2.58491354, 2.17642315, 2.16295473, 2.16946809, 2.30026683,\n",
       "       1.84346195, 3.01840632, 1.91912128, 2.22891139, 2.34430185,\n",
       "       2.17164242, 1.96847954, 2.03872053, 1.49565959, 2.12544882,\n",
       "       1.60598913, 3.05611058, 2.33681508, 2.72985695, 2.19074525,\n",
       "       2.38552801, 3.14762165, 2.07939576, 3.87240407, 2.12529453,\n",
       "       2.68252715, 2.75262509, 3.14527497, 2.1357986 , 1.97313418,\n",
       "       1.97886125, 2.47731603, 1.71775282, 2.67570283, 2.33899114,\n",
       "       1.43302893, 2.1265403 , 2.11053024, 2.12487543, 2.83589965,\n",
       "       3.23404768, 2.16682042, 2.8611302 , 2.16619186, 2.188097  ,\n",
       "       2.56486772, 2.6035642 , 2.12270947, 3.2283667 , 2.93059157,\n",
       "       2.07337787, 2.1818839 , 1.56153988, 2.59440929, 3.20402969,\n",
       "       2.29853214, 2.14385351, 2.52529423, 1.57306899, 2.29545637,\n",
       "       2.04298772, 3.11001779, 2.28301766, 1.66623161, 1.70746435,\n",
       "       2.21132927, 2.36281801, 3.93232591, 1.74738276, 1.97013355,\n",
       "       2.52838516, 2.53075342, 2.84751752, 3.15653347, 2.28288922,\n",
       "       1.81530707, 2.75569495, 2.49824249, 1.87618791, 1.83016238,\n",
       "       1.89588382, 2.63994252, 1.63723124, 1.87941436, 2.11660471,\n",
       "       1.13679226, 3.93515107, 2.4243001 , 1.95293106, 1.84868002,\n",
       "       2.34299248, 2.57124299, 2.40800824, 2.29173558, 2.02906034,\n",
       "       4.25000455, 2.96706848, 1.9686941 , 2.78680134, 2.99753909,\n",
       "       1.29620707, 3.58312051, 2.54574482, 2.14385723, 1.85369378,\n",
       "       2.02107201, 2.23119411, 3.57167882, 1.84800338, 2.99658205,\n",
       "       1.86581403, 2.30880681, 1.38911154, 2.4558484 , 1.58719591,\n",
       "       2.79405752, 2.43173761, 2.03785426, 1.74047091, 2.34282296,\n",
       "       2.56099158, 2.8997419 , 2.62112153, 2.65335173, 1.78385798,\n",
       "       2.2639949 , 2.34244448, 2.69646021, 2.44525328, 2.32525038,\n",
       "       3.85956038, 2.19850035, 1.64832149, 2.53069963, 2.78842195,\n",
       "       2.67185595, 2.10590542, 1.84305032, 1.72230191, 2.26516814,\n",
       "       1.29186912, 1.47197747, 3.12710733, 2.36828555, 1.69676842,\n",
       "       1.71843166, 2.7986262 , 1.90368694, 2.74276918, 2.23367293,\n",
       "       2.62732293, 2.473197  , 3.0316182 , 3.41994087, 2.82625044,\n",
       "       1.92079525, 2.26717407, 2.35185276, 2.56591396, 2.94199717,\n",
       "       2.41011008, 1.81278243, 2.75063183, 1.30612624, 2.59399554,\n",
       "       1.90604076, 2.84905955, 2.86413283, 2.53622532, 1.74417858,\n",
       "       2.03648958, 2.21191557, 2.46162901, 3.42181908, 3.0906915 ,\n",
       "       2.52815841, 1.59477112, 1.8558649 , 2.0144387 , 2.40763645,\n",
       "       2.9741304 , 2.85159908, 3.11864034, 1.6313069 , 2.35379706,\n",
       "       2.85399732, 2.81394984, 2.11342355, 1.66261144, 1.68805457,\n",
       "       2.19201593, 2.1864173 , 2.61686116, 2.15785121, 2.60753834,\n",
       "       2.25064075, 2.66425092, 2.37442463, 2.25893163, 1.95446558,\n",
       "       1.26596597, 2.95782852, 2.18942658, 2.68636312, 2.07990821,\n",
       "       1.53338094, 2.85724476, 1.65850959, 2.49004378, 3.09242987,\n",
       "       1.79144359, 3.34730316, 2.74963772, 3.98851214, 1.85345671,\n",
       "       2.05730212, 2.66383441, 2.56525339, 2.19837976, 2.70276896,\n",
       "       2.35023105, 2.39532078, 2.26593191, 2.15986074, 2.24697135,\n",
       "       2.19662635, 1.96478164, 2.30284867, 2.14407066, 2.16355633,\n",
       "       1.83691529, 4.65740733, 2.24186703, 2.19388283, 1.91030511,\n",
       "       1.62490772, 1.80622821, 2.18595063, 2.66174779, 2.56591607,\n",
       "       3.08673767, 2.1658475 , 3.10306593, 2.04906534, 2.41592217,\n",
       "       2.34209311, 2.36432013, 1.55955561, 1.98217243, 2.69872174,\n",
       "       2.43190986, 2.32094507, 2.19371158, 2.29047376, 3.0082994 ,\n",
       "       1.7085586 , 1.43636316, 1.64699873, 1.66284447, 2.19386626,\n",
       "       2.30879143, 2.49549891, 2.5767857 , 2.90507466, 2.57401656,\n",
       "       2.06064174, 1.76000505, 2.81894428, 2.24523171, 2.32142183,\n",
       "       1.89579428, 1.57077359, 1.86485128, 2.12341981, 2.428618  ,\n",
       "       2.38239644, 1.62534848, 2.97385084, 2.25950425, 3.7706223 ,\n",
       "       3.48195242, 3.23024946, 2.69565375, 1.89017386, 1.96631223,\n",
       "       2.33318057, 2.25957361, 1.96302777, 1.79790835, 1.65318706,\n",
       "       2.32094507, 1.48177662, 1.93416217, 2.84524513, 3.20951049,\n",
       "       2.95807338, 2.65314547, 1.94303875, 2.03510542, 2.7715401 ,\n",
       "       2.2771722 , 2.62339981, 1.59021008, 2.09688802, 2.53693156,\n",
       "       1.88098567, 2.33218058, 1.90084261, 1.7945158 , 1.734761  ,\n",
       "       2.82092716, 1.69464405, 2.01015569, 2.27845714, 2.96893011,\n",
       "       1.59797365, 1.88542504, 1.85471704, 2.51031404, 2.10555975,\n",
       "       2.47887109, 2.10604308, 1.72312598, 2.7056999 , 3.10719646,\n",
       "       2.43831647, 1.8350132 , 1.62481424, 1.53401391, 2.05916748,\n",
       "       2.23179389, 1.66179896, 3.18702468, 1.8436734 , 2.15422734,\n",
       "       2.98651448, 2.09531872, 1.65775986, 1.70344717, 2.64460068,\n",
       "       3.36458462, 2.49417206, 2.07174502, 2.40800237, 1.92761201,\n",
       "       2.37500688, 1.63595418, 1.64582047, 1.62844789, 1.95777977,\n",
       "       2.54493582, 2.91854548, 2.61194994, 2.38128118, 2.30607957,\n",
       "       3.32197741, 1.67908601, 2.82744609, 2.48961499, 2.4397357 ,\n",
       "       1.92934265, 2.75768382, 3.48158925, 1.45251052, 3.37777201,\n",
       "       3.32532616, 1.94216052, 2.09719622, 1.75185206, 1.74279543,\n",
       "       3.64889348, 1.69906007, 2.48558875, 2.65518986, 2.99124808,\n",
       "       1.74201066, 2.23333725, 2.10658526, 2.69520421, 2.51260632,\n",
       "       1.57487237, 1.44086067, 3.22814029, 2.93555201, 3.05920795,\n",
       "       2.96799548, 1.73583684, 2.0463459 , 2.8611039 , 2.92908131,\n",
       "       2.31293183, 2.54776606, 1.93790671, 3.00613701, 1.81978935,\n",
       "       2.76178211, 2.80873166, 2.83278611, 2.6393784 , 3.47910753,\n",
       "       2.40288414, 2.05460247, 1.541508  , 1.6804596 , 2.16025083,\n",
       "       2.74130137, 2.42666279, 1.61635155, 2.1364737 , 2.45354265,\n",
       "       1.94168875, 2.2454598 , 2.47237949, 1.7764286 , 1.9497823 ,\n",
       "       2.69093929, 2.11042328, 2.72156062, 2.41861893, 2.16866209,\n",
       "       2.19902261, 2.19461333, 2.00199437, 2.09009339, 2.93322108,\n",
       "       2.08822932, 2.51718691, 2.15654216, 2.86131115, 1.81622309,\n",
       "       2.23103391, 2.60807817, 1.84178853, 1.49916011, 2.22400066,\n",
       "       1.48640534, 2.81524008, 1.89665652, 2.02072824, 2.3204676 ])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg1 = GradientBoostingRegressor()\n",
    "reg1.fit(X_train, Y_train)\n",
    "Y_gradient_boosting_pred = reg1.predict(X_test)\n",
    "Y_gradient_boosting_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28917043098225714"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsquar2 = r2_score(Y_test, Y_gradient_boosting_pred)\n",
    "Rsquar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.30402444, 2.97312716, 3.04189372, 1.70620312, 2.1354707 ,\n",
       "       1.93313913, 1.90101804, 1.7155    , 4.16805069, 2.53409298,\n",
       "       2.18151814, 2.30171856, 2.74600305, 1.87442727, 1.93314951,\n",
       "       4.96105   , 2.0361    , 3.56809795, 1.5709572 , 2.1543    ,\n",
       "       2.20712743, 2.53123084, 1.62024204, 2.5605    , 1.91499183,\n",
       "       1.82803603, 2.5638    , 2.01541299, 2.56329759, 1.9645572 ,\n",
       "       2.62655071, 3.19625404, 2.3598241 , 2.80374145, 2.19155   ,\n",
       "       2.3219554 , 2.72615155, 1.44301903, 3.01885   , 2.38354428,\n",
       "       2.52470866, 4.0792626 , 1.6403    , 2.50361874, 2.62026541,\n",
       "       4.78715061, 3.40143835, 3.07648958, 1.4784    , 2.2422478 ,\n",
       "       2.21596095, 2.1694    , 1.65713336, 2.16221417, 1.3521    ,\n",
       "       1.4702    , 3.5057054 , 1.6819572 , 1.7172    , 1.89127828,\n",
       "       5.02267327, 1.4691    , 2.13953391, 2.41761466, 3.57113235,\n",
       "       2.18545015, 1.7411    , 1.73195   , 1.94046864, 1.4425    ,\n",
       "       2.04200312, 3.18541821, 2.68191422, 2.1158    , 2.73918029,\n",
       "       2.05008336, 2.1633    , 1.82067365, 1.7721    , 1.89295373,\n",
       "       3.03963057, 3.29047118, 2.2506    , 3.0676    , 2.25350601,\n",
       "       2.14265413, 2.92695784, 2.63833024, 2.08964446, 2.14590918,\n",
       "       1.6449    , 3.38803367, 3.27117301, 2.21613336, 3.17498614,\n",
       "       4.35495   , 2.244     , 2.76      , 3.20864686, 2.64869676,\n",
       "       3.20488019, 1.701     , 2.1062    , 2.32718037, 1.6156    ,\n",
       "       2.99535362, 1.43829709, 2.16351158, 3.00170803, 2.12842456,\n",
       "       2.8838263 , 2.52670398, 2.29679664, 3.19913868, 1.80795   ,\n",
       "       1.7151    , 3.05224903, 1.8850454 , 2.721     , 2.03644951,\n",
       "       2.1428    , 2.86418415, 3.08332074, 3.47331836, 2.1373    ,\n",
       "       2.1361    , 2.28494845, 2.68166007, 1.4946    , 2.34923502,\n",
       "       3.44996889, 1.98429183, 2.17822717, 2.2145    , 1.85625   ,\n",
       "       1.5635    , 1.6569    , 3.14681871, 2.35608531, 2.0227    ,\n",
       "       1.50238412, 2.1189    , 2.68996141, 2.64172482, 1.6193    ,\n",
       "       2.35541794, 2.05936389, 2.16404854, 1.6890572 , 2.53811049,\n",
       "       1.6992    , 1.4021    , 3.44437518, 3.21169821, 2.48028233,\n",
       "       1.3829    , 3.88070129, 2.24772717, 2.93305797, 1.4301    ,\n",
       "       1.4875    , 2.0945    , 3.04060796, 2.49627773, 2.55715643,\n",
       "       1.8472    , 2.5019    , 2.14392952, 1.71695726, 3.22984909,\n",
       "       1.82913246, 2.81225   , 3.37290146, 1.761     , 1.443     ,\n",
       "       3.20801787, 2.73039638, 4.17508367, 2.31997115, 2.60771133,\n",
       "       2.1358    , 4.09150168, 1.4434    , 1.344     , 2.41627565,\n",
       "       4.01685378, 2.40973571, 4.13580619, 2.8907114 , 2.40139587,\n",
       "       2.1392    , 2.54497663, 1.7342    , 1.6381    , 1.7014553 ,\n",
       "       2.8416217 , 1.8939    , 2.51930817, 1.51173571, 1.6011    ,\n",
       "       2.06144324, 3.28533985, 2.0596    , 2.72841172, 1.233     ,\n",
       "       2.38060444, 1.88524204, 1.99874951, 1.7885055 , 2.43169251,\n",
       "       2.64638544, 3.70855869, 1.91407078, 1.8197    , 3.05599861,\n",
       "       2.57266316, 1.81859183, 1.7633    , 3.92922611, 1.8348    ,\n",
       "       2.19135533, 1.3852    , 1.809     , 2.7460263 , 1.58825   ,\n",
       "       1.7955    , 2.77982053, 1.85685   , 2.41605855, 1.784     ,\n",
       "       2.0552    , 1.6144    , 1.8795    , 1.8962    , 2.68310268,\n",
       "       1.3084    , 1.2551    , 2.41567623, 2.59326278, 3.0553311 ,\n",
       "       1.7158    , 2.95778633, 4.73252532, 1.73771391, 1.8715    ,\n",
       "       5.04845391, 2.87511408, 2.3912    , 1.69024204, 2.9868    ,\n",
       "       1.2918    , 3.10200749, 2.17786205, 4.18995154, 1.8916    ,\n",
       "       2.51972528, 2.24452767, 1.5901    , 3.99700182, 2.36336176,\n",
       "       3.73710185, 1.3993    , 3.2014    , 2.9621    , 1.52235602,\n",
       "       1.8168    , 3.31952526, 2.93820867, 2.64956005, 2.54918922,\n",
       "       1.73120398, 1.6852    , 4.78995194, 2.54609357, 2.7174889 ,\n",
       "       1.7424    , 3.20879524, 2.24337831, 1.7449    , 1.89608469,\n",
       "       1.3171    , 2.50885806, 4.11245   , 2.60541996, 1.9367    ,\n",
       "       2.48501781, 2.10047078, 2.09598366, 1.6647    , 3.39169796,\n",
       "       2.33815215, 1.96005   , 2.23544437, 2.99187274, 1.3032    ,\n",
       "       2.36055575, 2.39605709, 2.47498297, 2.01334641, 1.4824    ,\n",
       "       2.91540402, 1.9551375 , 4.40746645, 2.96585   , 3.12130158,\n",
       "       3.68824796, 2.4583866 , 1.93725049, 1.3335    , 2.37476364,\n",
       "       2.26443847, 2.78715624, 4.55877291, 2.6389473 , 1.64369059,\n",
       "       1.3761    , 1.69744753, 2.13068648, 2.86424105, 2.17210776,\n",
       "       2.0986    , 3.02153508, 2.8607    , 2.0958    , 1.92937403,\n",
       "       2.2598    , 2.63149644, 1.9474    , 1.8408    , 3.30673347,\n",
       "       2.3439    , 3.95806179, 1.8441572 , 2.68675   , 2.8556    ,\n",
       "       1.6103    , 3.52702715, 2.093     , 2.62728699, 1.7009    ,\n",
       "       2.47726952, 3.5455    , 2.2522    , 3.0179    , 2.88010254,\n",
       "       3.19175   , 1.5604    , 2.53238417, 3.1101    , 2.00628196,\n",
       "       2.09900342, 1.97658469, 1.3797    , 2.0041572 , 3.00752301,\n",
       "       2.5426958 , 1.8666    , 1.76761932, 1.55335   , 1.95459183,\n",
       "       1.92992456, 1.90206032, 2.09591275, 2.54428961, 2.01354204,\n",
       "       2.18056949, 2.34985963, 1.6789    , 4.23199355, 2.10958549,\n",
       "       1.4269    , 1.7578    , 2.06717024, 2.5023422 , 2.2431    ,\n",
       "       2.11482734, 3.33508637, 2.90376315, 2.2954967 , 2.01662717,\n",
       "       2.24023585, 1.74992078, 1.5227    , 1.65701847, 2.62055146,\n",
       "       1.513     , 3.30381057, 2.8637    , 1.95917856, 2.63173979,\n",
       "       2.02371069, 1.67076967, 1.8459    , 1.4427    , 1.88188701,\n",
       "       1.7445    , 2.80039292, 2.92392999, 2.52168702, 1.90683947,\n",
       "       1.7368    , 2.53908052, 2.3387    , 5.05976468, 1.74585082,\n",
       "       3.52553422, 3.07763053, 2.71100784, 2.38495963, 2.01395817,\n",
       "       2.095     , 2.86085817, 1.6338    , 2.9199019 , 2.24875322,\n",
       "       1.4263    , 1.94585   , 1.8494    , 2.18654105, 3.09210466,\n",
       "       3.11522345, 2.3392    , 1.82275309, 1.86847735, 1.86695146,\n",
       "       2.84402086, 2.0628    , 2.19498502, 3.33848375, 2.69992336,\n",
       "       2.17162905, 2.12470819, 1.6269    , 3.12503864, 3.88885274,\n",
       "       2.28405   , 1.87990194, 3.12109289, 1.6614572 , 2.1096    ,\n",
       "       1.94455049, 2.70778062, 2.02397782, 2.0865    , 1.5662    ,\n",
       "       1.67218182, 4.42347221, 4.2515266 , 1.6862    , 1.7372    ,\n",
       "       2.48138455, 2.73922572, 2.31075   , 2.89581606, 1.8291    ,\n",
       "       1.6823    , 2.9551    , 2.8637573 , 1.793     , 1.8963    ,\n",
       "       1.3887    , 2.51917744, 1.72620717, 1.7977    , 2.0832    ,\n",
       "       1.2509    , 4.40526091, 2.58321374, 1.8878    , 2.03637928,\n",
       "       1.90444183, 2.71188539, 1.96840281, 2.13872094, 1.96563608,\n",
       "       3.94288747, 3.10503239, 1.8296    , 3.10156514, 3.54100135,\n",
       "       1.5521    , 3.43138885, 2.29095036, 1.8243    , 2.29596673,\n",
       "       1.86958408, 2.54192836, 4.40323442, 1.254     , 3.18807189,\n",
       "       1.99309009, 2.0839    , 1.4155    , 2.62997316, 1.447     ,\n",
       "       2.89250819, 2.7963    , 1.58675373, 1.6964    , 1.8098    ,\n",
       "       2.85247589, 3.2789194 , 3.73295008, 3.4619745 , 1.80938412,\n",
       "       1.8001    , 2.09350264, 3.04733879, 2.01805398, 3.54010228,\n",
       "       4.3588855 , 1.63215272, 1.295     , 2.2407    , 3.44287865,\n",
       "       2.35039158, 2.1946    , 1.96941607, 1.51773806, 2.0135    ,\n",
       "       1.3573    , 1.3813    , 3.58719055, 2.00780776, 1.0758    ,\n",
       "       1.6794    , 2.2979641 , 2.0645    , 2.80095255, 1.70241843,\n",
       "       1.9602306 , 2.9282042 , 3.72555803, 3.78866886, 3.18218159,\n",
       "       2.0895    , 1.56653728, 2.54905413, 1.79830817, 3.66330306,\n",
       "       2.21635097, 2.23845   , 2.81630217, 1.2671572 , 3.52512366,\n",
       "       2.08781664, 2.88038311, 2.86173102, 2.55997875, 2.31969883,\n",
       "       2.8449    , 2.3768    , 2.4382    , 3.25076264, 3.29837154,\n",
       "       3.00146526, 1.6196    , 1.83375049, 1.9441    , 2.99690988,\n",
       "       3.00051084, 3.34267123, 3.25113398, 1.71801903, 1.71427468,\n",
       "       2.317     , 2.96908702, 2.34758181, 1.95503922, 1.4975    ,\n",
       "       1.82279183, 2.81263456, 2.72064575, 2.20231903, 2.17296624,\n",
       "       1.98409124, 1.9683353 , 2.78908762, 2.49441403, 2.33202717,\n",
       "       1.3863    , 2.8438    , 1.92535   , 3.43551578, 2.3021875 ,\n",
       "       1.3049    , 2.9994253 , 1.7071    , 1.88388064, 3.72047094,\n",
       "       1.6073    , 3.7164583 , 2.61645   , 3.88194142, 1.96557782,\n",
       "       2.45228102, 2.92453416, 2.87399251, 1.95170817, 2.85971645,\n",
       "       2.42041215, 2.48784139, 2.53051962, 2.4552    , 2.13680214,\n",
       "       2.12415999, 1.2356    , 2.27713868, 1.80749707, 2.29085654,\n",
       "       1.5855    , 4.71993848, 1.4714    , 1.5292    , 1.4197    ,\n",
       "       1.9964572 , 1.91522777, 2.9123    , 3.25039025, 1.44659105,\n",
       "       2.94789401, 1.8957    , 3.62243849, 2.06107316, 1.91771903,\n",
       "       2.59788729, 2.47326695, 1.92743142, 1.90225488, 2.17705229,\n",
       "       3.0224    , 2.1527    , 2.06632823, 1.83      , 3.1955711 ,\n",
       "       1.7743    , 1.2911    , 1.7431    , 1.7186    , 2.37932803,\n",
       "       2.161     , 2.73304086, 2.52867301, 2.57718563, 2.17497667,\n",
       "       2.1587    , 1.47838609, 2.87489727, 1.8074    , 1.97686316,\n",
       "       2.62758152, 1.7641    , 2.11104204, 1.6658    , 2.17951441,\n",
       "       2.13562237, 1.9472    , 2.73067571, 2.0383    , 3.95309111,\n",
       "       3.89076016, 3.02565719, 4.01391971, 1.8891    , 1.78891903,\n",
       "       2.1755    , 1.99855   , 2.15460871, 1.58356864, 2.1949    ,\n",
       "       2.1001    , 1.5082    , 2.0221375 , 2.9533487 , 3.15472517,\n",
       "       3.41278388, 2.22168102, 1.5465    , 1.86404951, 2.9166    ,\n",
       "       2.08110291, 1.63903978, 1.73800887, 2.5225    , 2.7305    ,\n",
       "       2.56202517, 3.03260803, 2.27887749, 1.7038    , 1.5452    ,\n",
       "       2.67406107, 1.5561    , 1.3515    , 2.42930151, 2.57129835,\n",
       "       1.6243    , 1.4821    , 1.51274784, 2.84937285, 1.94634455,\n",
       "       1.4592    , 1.55728549, 1.772     , 2.54703051, 3.45223066,\n",
       "       2.91561789, 2.0715    , 1.7093    , 1.4698    , 2.31074563,\n",
       "       2.33804638, 1.8897    , 4.22772197, 2.8953    , 1.993     ,\n",
       "       3.14070796, 1.8074    , 1.92754854, 1.6727    , 2.11088102,\n",
       "       2.36294716, 2.66147684, 1.91722264, 2.8217    , 2.15641612,\n",
       "       2.32298415, 1.92578526, 1.4107    , 1.8595    , 2.23476226,\n",
       "       3.11190229, 3.44045495, 2.38730413, 2.15596091, 3.44885828,\n",
       "       2.76418493, 2.0304    , 3.0198541 , 2.35052078, 2.30557868,\n",
       "       1.45719602, 3.51956661, 3.31833034, 1.1856    , 3.481257  ,\n",
       "       3.08450216, 2.0557    , 1.9326    , 1.76676119, 1.7448    ,\n",
       "       3.51911357, 1.2851    , 2.54335735, 2.99009688, 3.19431787,\n",
       "       1.3974    , 2.0091    , 2.80593412, 3.07459613, 2.12431705,\n",
       "       1.6422    , 1.4718    , 3.57936136, 4.39090398, 2.2851474 ,\n",
       "       2.85568386, 1.7606    , 2.05160695, 2.28576451, 3.14609364,\n",
       "       1.71742843, 3.5594    , 2.0161    , 3.66239212, 1.6062    ,\n",
       "       2.70116036, 3.02535798, 2.97686507, 1.70940639, 3.97135601,\n",
       "       1.9734    , 2.30348449, 1.3196    , 1.5378    , 1.9412    ,\n",
       "       2.9740678 , 2.41220908, 1.7057    , 1.85873135, 2.49445989,\n",
       "       2.03984155, 1.7107    , 2.15601069, 2.13354127, 1.6       ,\n",
       "       2.60094964, 2.28530371, 2.33617285, 2.45820888, 2.27449931,\n",
       "       1.87512904, 2.51976122, 1.75143395, 2.23783336, 2.73518834,\n",
       "       2.13979102, 1.88286805, 2.91265   , 2.4483568 , 1.6611    ,\n",
       "       1.68518196, 1.97825138, 1.93755872, 1.5777144 , 1.98678701,\n",
       "       1.2975    , 3.68177649, 1.4511    , 1.86369687, 2.99692468])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(X_train, Y_train)\n",
    "Y_pred3 = reg.predict(X_test)\n",
    "Y_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.428138154292272"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsquar3 = r2_score(Y_test, Y_pred3)\n",
    "Rsquar3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:34:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:34:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.1.0/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.8074054 , 2.0372105 , 2.8703194 , 1.7474418 , 2.11903   ,\n",
       "       1.898176  , 2.3773587 , 1.8404568 , 4.006625  , 3.2288501 ,\n",
       "       2.167879  , 1.9807059 , 3.2065053 , 1.4703677 , 2.1127954 ,\n",
       "       4.4630938 , 2.807105  , 3.184174  , 1.6717225 , 2.5235162 ,\n",
       "       2.822292  , 2.1689856 , 1.5430977 , 2.3431797 , 2.7626252 ,\n",
       "       2.1570718 , 2.498592  , 2.3746252 , 2.1378312 , 1.4615886 ,\n",
       "       3.1701114 , 3.1914718 , 2.7760968 , 2.3849912 , 2.1583285 ,\n",
       "       2.4461384 , 2.84029   , 1.5611644 , 2.7287347 , 1.8419098 ,\n",
       "       2.950322  , 3.7614791 , 1.7181052 , 2.1208453 , 2.2843938 ,\n",
       "       4.6628156 , 3.5422654 , 2.6944718 , 2.102688  , 1.9330357 ,\n",
       "       2.0026698 , 1.9741555 , 1.7545205 , 2.1953282 , 1.2007606 ,\n",
       "       1.6858649 , 4.096322  , 1.5284605 , 1.8553308 , 1.9735683 ,\n",
       "       4.8212323 , 1.4952637 , 1.6792432 , 1.1630311 , 3.794844  ,\n",
       "       1.6388142 , 1.6088564 , 1.5987711 , 2.8669827 , 1.5831815 ,\n",
       "       1.8751612 , 2.5494144 , 2.2723503 , 2.703618  , 1.6890398 ,\n",
       "       2.1900616 , 2.0896976 , 2.0526388 , 1.2374294 , 1.4579666 ,\n",
       "       3.2169995 , 3.3765793 , 1.8747422 , 2.9655857 , 2.50457   ,\n",
       "       2.8595307 , 2.419441  , 2.5478675 , 1.7811159 , 1.5971189 ,\n",
       "       1.9246002 , 2.950873  , 3.4067712 , 2.361089  , 2.3981209 ,\n",
       "       3.5225744 , 2.5566406 , 1.7961015 , 2.9051795 , 2.6510036 ,\n",
       "       3.0528    , 1.6185548 , 1.3880737 , 2.9863229 , 1.7122746 ,\n",
       "       3.091877  , 1.8542471 , 2.5856729 , 3.184801  , 1.6704376 ,\n",
       "       2.9078999 , 2.1666365 , 2.7524614 , 2.7117512 , 2.8831992 ,\n",
       "       1.4517889 , 3.3518054 , 2.776918  , 2.8371227 , 1.5625386 ,\n",
       "       1.5716405 , 2.7485628 , 3.032879  , 3.305427  , 2.323709  ,\n",
       "       1.60218   , 2.1834924 , 2.9674194 , 1.6136283 , 2.2965055 ,\n",
       "       2.7758212 , 2.486462  , 2.168655  , 2.1100392 , 2.5298395 ,\n",
       "       1.1648285 , 1.9681414 , 2.888729  , 2.2600958 , 2.4028492 ,\n",
       "       2.443151  , 1.83173   , 2.7388284 , 2.4115927 , 2.0705822 ,\n",
       "       1.9945974 , 1.9115193 , 1.6589601 , 2.3056111 , 2.5690117 ,\n",
       "       1.1935967 , 1.1467116 , 2.7242665 , 2.7562928 , 2.4764943 ,\n",
       "       1.3185523 , 4.0818987 , 2.2340183 , 4.0308146 , 1.6766912 ,\n",
       "       1.0783893 , 2.131881  , 2.7469335 , 2.3766313 , 2.5759954 ,\n",
       "       2.182251  , 2.1461883 , 2.5937018 , 2.000674  , 2.8620076 ,\n",
       "       2.2265565 , 3.2625928 , 3.5724008 , 1.652251  , 1.4374964 ,\n",
       "       2.6085947 , 2.7959714 , 3.627792  , 2.225844  , 2.220982  ,\n",
       "       2.8713038 , 4.3838005 , 1.7521662 , 1.4850748 , 2.6556458 ,\n",
       "       4.0584373 , 3.1538796 , 3.4075863 , 3.296742  , 2.171576  ,\n",
       "       2.6563172 , 3.0180008 , 2.0540519 , 1.829747  , 2.9538865 ,\n",
       "       2.1066985 , 2.2021499 , 2.4646134 , 1.5869094 , 1.5663137 ,\n",
       "       1.9347522 , 4.6732736 , 2.5313427 , 3.0386975 , 1.3384333 ,\n",
       "       1.8523605 , 1.8797385 , 2.1005702 , 2.1344194 , 2.103344  ,\n",
       "       2.5617814 , 3.9699323 , 2.2019796 , 0.6808033 , 2.2175436 ,\n",
       "       2.0560348 , 1.7159362 , 1.5005686 , 3.174044  , 1.9724298 ,\n",
       "       1.8764678 , 1.754831  , 2.2873552 , 3.114662  , 1.6197932 ,\n",
       "       1.5797126 , 2.9455483 , 1.4775784 , 1.7788249 , 1.6033022 ,\n",
       "       2.4162989 , 1.7677369 , 1.7668111 , 2.1102774 , 2.8304732 ,\n",
       "       2.0349631 , 1.3022411 , 2.057507  , 2.2156363 , 2.1167343 ,\n",
       "       1.1102535 , 3.478681  , 4.663053  , 2.144236  , 2.3459682 ,\n",
       "       5.2902484 , 2.4895759 , 2.0525894 , 1.2688594 , 2.9484463 ,\n",
       "       1.3137561 , 3.4310262 , 2.3893528 , 3.7357233 , 2.0914419 ,\n",
       "       3.0319607 , 1.4785087 , 1.4553709 , 3.9968605 , 2.6678722 ,\n",
       "       3.4602153 , 0.96046966, 3.4702396 , 2.8961322 , 1.1005247 ,\n",
       "       1.8720267 , 3.2367773 , 2.1831534 , 2.1502686 , 2.2470903 ,\n",
       "       1.6791023 , 1.4362948 , 5.983758  , 2.4685607 , 3.3592715 ,\n",
       "       2.0520093 , 3.9243433 , 2.8972359 , 1.3997744 , 2.4651842 ,\n",
       "       1.5586838 , 2.4612923 , 3.7064855 , 2.6713202 , 2.2719789 ,\n",
       "       2.2988067 , 2.1406608 , 2.3250904 , 1.0690911 , 3.2843585 ,\n",
       "       2.190283  , 2.4055123 , 2.479093  , 2.3253856 , 1.207904  ,\n",
       "       2.4271054 , 2.6075337 , 2.04828   , 1.7375087 , 1.2715861 ,\n",
       "       3.012201  , 2.040255  , 4.360979  , 2.3267403 , 2.7903278 ,\n",
       "       3.331325  , 2.5900085 , 2.1086931 , 1.4248884 , 2.547715  ,\n",
       "       2.100097  , 2.047356  , 5.264852  , 2.9721904 , 1.3190043 ,\n",
       "       1.5598645 , 1.8876086 , 2.494932  , 4.2056255 , 2.2620914 ,\n",
       "       1.9144149 , 2.8044403 , 2.7154205 , 2.3213902 , 2.3652582 ,\n",
       "       2.4868236 , 2.9988616 , 2.81635   , 1.511463  , 2.0323832 ,\n",
       "       2.4672403 , 3.2809055 , 1.3281239 , 2.1417756 , 2.497064  ,\n",
       "       2.1869981 , 2.3694854 , 2.3392587 , 2.9694855 , 1.3786776 ,\n",
       "       2.417037  , 3.00515   , 2.2308226 , 3.020651  , 2.8017557 ,\n",
       "       3.9058518 , 1.4974248 , 1.9136087 , 3.2751188 , 1.8818889 ,\n",
       "       2.1103797 , 1.9822993 , 1.2486391 , 1.557265  , 2.4289382 ,\n",
       "       2.0508761 , 2.054449  , 1.2823644 , 1.269844  , 2.2046304 ,\n",
       "       1.6001306 , 1.9510163 , 2.0829818 , 2.0222118 , 1.4177566 ,\n",
       "       2.6195045 , 2.4516697 , 2.0864148 , 4.9189196 , 1.42191   ,\n",
       "       1.5655153 , 2.0870914 , 1.8948258 , 1.330659  , 3.5097573 ,\n",
       "       1.9737589 , 2.92976   , 2.1354792 , 2.0638046 , 1.4709754 ,\n",
       "       2.57785   , 1.5347592 , 1.6124718 , 1.4686401 , 2.7392066 ,\n",
       "       1.545846  , 3.0325267 , 2.9287567 , 2.1942873 , 2.123309  ,\n",
       "       1.7816191 , 2.2255378 , 1.6538209 , 1.4304304 , 1.7040896 ,\n",
       "       1.8482673 , 2.3791323 , 1.9243695 , 2.5380833 , 2.2028496 ,\n",
       "       1.4473774 , 2.4573092 , 2.5523238 , 4.1541033 , 1.8295817 ,\n",
       "       2.837235  , 2.359686  , 2.6396894 , 2.0848317 , 2.750897  ,\n",
       "       2.0878808 , 3.419012  , 1.5103006 , 2.870972  , 1.8870412 ,\n",
       "       1.1345689 , 2.346889  , 2.1765158 , 1.7620773 , 3.3031363 ,\n",
       "       3.8871195 , 2.2048054 , 2.3918188 , 1.8595648 , 2.160193  ,\n",
       "       3.1106799 , 1.724934  , 2.1647358 , 3.145791  , 3.4352093 ,\n",
       "       2.3718524 , 2.7072523 , 1.7189014 , 2.4116488 , 4.0801344 ,\n",
       "       1.7921267 , 1.572855  , 3.05509   , 1.5753917 , 2.2879121 ,\n",
       "       2.265009  , 3.3296912 , 2.032878  , 2.0258956 , 1.5088412 ,\n",
       "       1.5310079 , 2.9086676 , 4.2739644 , 1.9826516 , 1.6941965 ,\n",
       "       2.674107  , 2.2866    , 2.6192226 , 2.645313  , 2.5141995 ,\n",
       "       1.8031304 , 3.1225123 , 2.6428518 , 1.584853  , 1.9360379 ,\n",
       "       1.5919056 , 2.89646   , 1.8260586 , 1.2824099 , 2.2033734 ,\n",
       "       1.2415209 , 4.924027  , 2.213665  , 1.8658384 , 2.5425804 ,\n",
       "       2.2725878 , 2.4867258 , 2.3020248 , 2.3464677 , 1.4795063 ,\n",
       "       4.4778137 , 3.375973  , 2.0625477 , 3.2325544 , 2.8966053 ,\n",
       "       1.2789166 , 3.5165193 , 2.766136  , 2.248064  , 1.8968949 ,\n",
       "       1.9073313 , 2.0810938 , 3.7008903 , 1.4447818 , 3.5351884 ,\n",
       "       2.1517081 , 2.1470037 , 1.0527604 , 2.3920178 , 1.7147107 ,\n",
       "       2.8142161 , 1.5806866 , 1.303478  , 1.3553565 , 2.2075894 ,\n",
       "       2.2521353 , 2.275903  , 2.8912704 , 2.4677742 , 1.2787064 ,\n",
       "       2.1893795 , 2.6928272 , 3.0831044 , 2.1490273 , 2.7106965 ,\n",
       "       4.4551263 , 1.9376485 , 0.93539584, 2.330275  , 2.9629169 ,\n",
       "       2.301992  , 1.7147822 , 2.5159137 , 1.2267091 , 2.3784838 ,\n",
       "       1.5142411 , 1.6887851 , 3.585069  , 2.0585225 , 1.0199432 ,\n",
       "       1.7492347 , 2.0546484 , 1.9849036 , 2.5297577 , 1.7895468 ,\n",
       "       2.4429357 , 2.5358937 , 3.1029575 , 3.8668313 , 2.7496367 ,\n",
       "       1.7630967 , 1.8187095 , 2.514083  , 2.4659612 , 4.0040464 ,\n",
       "       2.5181012 , 1.4918776 , 2.6137784 , 1.1696498 , 2.3744392 ,\n",
       "       1.9537009 , 3.367287  , 3.48115   , 3.0474334 , 2.006576  ,\n",
       "       2.5334284 , 2.588746  , 2.4676688 , 3.4063826 , 3.398495  ,\n",
       "       2.2073526 , 1.7135664 , 1.8499224 , 2.4475102 , 2.8829    ,\n",
       "       3.4115565 , 3.2078094 , 3.8133893 , 2.007195  , 1.7202358 ,\n",
       "       3.8457694 , 3.5877135 , 1.32861   , 2.0267725 , 1.3186562 ,\n",
       "       1.6871176 , 2.852813  , 2.9057765 , 2.1833649 , 2.2833958 ,\n",
       "       2.3498042 , 2.5230706 , 2.156518  , 2.0597563 , 2.0359604 ,\n",
       "       1.0482104 , 3.113341  , 1.8127276 , 3.916977  , 2.3746364 ,\n",
       "       1.3761576 , 2.769904  , 1.3045353 , 2.268052  , 3.2526433 ,\n",
       "       2.5827317 , 3.4953632 , 2.900477  , 4.051663  , 1.975404  ,\n",
       "       2.195206  , 2.4730175 , 2.9840097 , 1.6837132 , 2.7435067 ,\n",
       "       2.1517086 , 2.5066893 , 2.6692145 , 2.030758  , 1.7362106 ,\n",
       "       2.2818813 , 1.7631692 , 1.9677145 , 2.510819  , 3.3209457 ,\n",
       "       0.82141507, 4.4948683 , 1.2479671 , 1.7126894 , 1.5293338 ,\n",
       "       2.0928264 , 1.3985159 , 2.5963004 , 2.697766  , 2.3255434 ,\n",
       "       3.4783497 , 2.3252366 , 3.3001485 , 1.7804742 , 1.7834412 ,\n",
       "       2.896194  , 2.1550188 , 1.2483957 , 2.1471307 , 2.0122468 ,\n",
       "       2.542874  , 1.7730793 , 2.0125794 , 2.168468  , 3.910837  ,\n",
       "       1.4305172 , 1.6997482 , 1.527117  , 1.6879107 , 1.6493629 ,\n",
       "       2.2852793 , 2.7306361 , 2.2192197 , 2.5507596 , 1.8508855 ,\n",
       "       2.0318217 , 1.6234322 , 3.7235944 , 1.1449404 , 2.334486  ,\n",
       "       1.6818264 , 1.2277156 , 2.4144914 , 1.3319619 , 2.1737094 ,\n",
       "       1.8147302 , 1.4498761 , 3.1039119 , 1.8408638 , 4.5205674 ,\n",
       "       3.8580256 , 3.3828976 , 2.7004948 , 1.7058275 , 1.8478765 ,\n",
       "       3.0664818 , 2.206296  , 1.571307  , 2.4298441 , 2.5760221 ,\n",
       "       2.4651465 , 1.249081  , 1.8589123 , 2.6591284 , 3.5407538 ,\n",
       "       3.4578893 , 2.357256  , 1.3621815 , 1.745722  , 2.1818871 ,\n",
       "       2.361732  , 1.8946972 , 1.5190353 , 2.4112773 , 2.4610472 ,\n",
       "       1.5270429 , 2.1957974 , 1.1389235 , 1.8596615 , 1.6061623 ,\n",
       "       2.075086  , 1.1931987 , 1.8336208 , 2.9108675 , 2.716749  ,\n",
       "       1.4553729 , 1.560906  , 1.7992103 , 2.8146758 , 1.8338057 ,\n",
       "       1.9325618 , 1.8723559 , 1.9558953 , 2.82877   , 3.2593658 ,\n",
       "       3.0073195 , 1.3856556 , 1.7981405 , 1.2940145 , 2.3273482 ,\n",
       "       2.5043    , 1.6763664 , 3.640912  , 2.779946  , 2.009429  ,\n",
       "       3.6715994 , 1.8614734 , 1.8203505 , 1.7406193 , 2.7858772 ,\n",
       "       2.9073617 , 3.2320232 , 2.208816  , 3.2822022 , 1.8319588 ,\n",
       "       1.6598947 , 1.4387832 , 1.5137286 , 1.7466785 , 1.8407819 ,\n",
       "       1.8938853 , 2.5987012 , 2.8008559 , 2.2448692 , 3.0734332 ,\n",
       "       3.3036156 , 2.246611  , 3.276703  , 3.2288136 , 2.6480143 ,\n",
       "       1.4179404 , 3.78161   , 3.6075277 , 1.5421207 , 3.6338496 ,\n",
       "       3.3772676 , 1.586582  , 2.5834036 , 1.9889705 , 1.5771816 ,\n",
       "       3.5703683 , 1.6171513 , 2.6968498 , 3.288323  , 3.4356217 ,\n",
       "       1.4633603 , 1.1556093 , 2.7232275 , 3.2446897 , 1.9575827 ,\n",
       "       1.3128111 , 0.97068423, 3.610121  , 3.1846256 , 2.8692303 ,\n",
       "       3.0351586 , 1.3037152 , 1.822865  , 2.4858284 , 1.9744651 ,\n",
       "       1.9306086 , 2.8613074 , 2.1201985 , 2.0067189 , 1.811633  ,\n",
       "       2.3239875 , 2.639032  , 3.314606  , 1.921493  , 3.6417198 ,\n",
       "       2.1597733 , 1.7470168 , 1.2825398 , 1.776065  , 2.1625175 ,\n",
       "       2.0258703 , 2.1517203 , 1.6106385 , 1.4905744 , 2.0983415 ,\n",
       "       2.9659    , 1.7983332 , 2.4499068 , 2.1504736 , 1.9542282 ,\n",
       "       2.7688725 , 2.4758286 , 2.4116125 , 1.9498019 , 2.0558853 ,\n",
       "       2.2273955 , 3.1649728 , 2.0178022 , 1.9866773 , 3.3127482 ,\n",
       "       1.975061  , 1.4110501 , 2.7633326 , 2.3525681 , 1.6702706 ,\n",
       "       2.8790357 , 2.20702   , 1.5734408 , 1.2534    , 1.8180728 ,\n",
       "       1.3217869 , 3.3347075 , 1.5087713 , 2.3267512 , 3.079001  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "Y_xgb_pred = xgb_model.predict(X_test)\n",
    "Y_xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3151828362680843"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsquar3 = r2_score(Y_test, Y_xgb_pred)\n",
    "Rsquar3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dir lstm w copier coller f dataset lakhor paliz <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 200\n",
    "batch_size = 32\n",
    "from keras.models import Sequential\n",
    "import tensorflow\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size =batch_size, nb_epoch = 1,  verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(x_test,y_test,verbose=2,batch_size=batch_size)\n",
    "print(\"score:%.2f \"%(score))\n",
    "print(\"Validation Accuracy:%.2f \"%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'RMSE': ['Belgium', 'India', 'Brazil'],\n",
    "'R': ['Brussels', 'New Delhi', 'Braslia']}\n",
    "df = pd.DataFrame(data,columns=['RMSE', 'R'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiLayer perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 4.8594\n",
      "Epoch 2/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4804\n",
      "Epoch 3/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3265\n",
      "Epoch 4/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2527\n",
      "Epoch 5/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2015\n",
      "Epoch 6/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1670\n",
      "Epoch 7/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1477\n",
      "Epoch 8/100\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.134 - 0s 3ms/step - loss: 1.1401\n",
      "Epoch 9/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1250\n",
      "Epoch 10/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.1134\n",
      "Epoch 11/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1040\n",
      "Epoch 12/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0987\n",
      "Epoch 13/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0915\n",
      "Epoch 14/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0898\n",
      "Epoch 15/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0829\n",
      "Epoch 16/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0777\n",
      "Epoch 17/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0677\n",
      "Epoch 18/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0697\n",
      "Epoch 19/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0578\n",
      "Epoch 20/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0538\n",
      "Epoch 21/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0636\n",
      "Epoch 22/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0508\n",
      "Epoch 23/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0474\n",
      "Epoch 24/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0382\n",
      "Epoch 25/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0345\n",
      "Epoch 26/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0345\n",
      "Epoch 27/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0342\n",
      "Epoch 28/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0289\n",
      "Epoch 29/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0236\n",
      "Epoch 30/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0263\n",
      "Epoch 31/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0224\n",
      "Epoch 32/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0179\n",
      "Epoch 33/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0178\n",
      "Epoch 34/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0173\n",
      "Epoch 35/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0160\n",
      "Epoch 36/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0074\n",
      "Epoch 37/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0022\n",
      "Epoch 38/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0072\n",
      "Epoch 39/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 1.0036\n",
      "Epoch 40/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0042\n",
      "Epoch 41/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0012\n",
      "Epoch 42/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9956\n",
      "Epoch 43/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.0032\n",
      "Epoch 44/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9953\n",
      "Epoch 45/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9960\n",
      "Epoch 46/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9888\n",
      "Epoch 47/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9906\n",
      "Epoch 48/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9966\n",
      "Epoch 49/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9902\n",
      "Epoch 50/100\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.983 - 0s 2ms/step - loss: 0.9892\n",
      "Epoch 51/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9844\n",
      "Epoch 52/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9794A: 0s - loss: 0.9\n",
      "Epoch 53/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9808\n",
      "Epoch 54/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9852\n",
      "Epoch 55/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9797\n",
      "Epoch 56/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9819\n",
      "Epoch 57/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9834\n",
      "Epoch 58/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9747\n",
      "Epoch 59/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9744\n",
      "Epoch 60/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9781\n",
      "Epoch 61/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9757\n",
      "Epoch 62/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9823\n",
      "Epoch 63/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9810\n",
      "Epoch 64/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9716\n",
      "Epoch 65/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9675A: 0s - loss: 0.\n",
      "Epoch 66/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9697\n",
      "Epoch 67/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9676\n",
      "Epoch 68/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9711\n",
      "Epoch 69/100\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.9832\n",
      "Epoch 70/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9678\n",
      "Epoch 71/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9658\n",
      "Epoch 72/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9606\n",
      "Epoch 73/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9678\n",
      "Epoch 74/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9648\n",
      "Epoch 75/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9593\n",
      "Epoch 76/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9614\n",
      "Epoch 77/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9587\n",
      "Epoch 78/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9676\n",
      "Epoch 79/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9582\n",
      "Epoch 80/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9595\n",
      "Epoch 81/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9596\n",
      "Epoch 82/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9594\n",
      "Epoch 83/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9621\n",
      "Epoch 84/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9548\n",
      "Epoch 85/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9582\n",
      "Epoch 86/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9583\n",
      "Epoch 87/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9563\n",
      "Epoch 88/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9527\n",
      "Epoch 89/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9562\n",
      "Epoch 90/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9517\n",
      "Epoch 91/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9602\n",
      "Epoch 92/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9493\n",
      "Epoch 93/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9465\n",
      "Epoch 94/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9458\n",
      "Epoch 95/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9513\n",
      "Epoch 96/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9446\n",
      "Epoch 97/100\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9462\n",
      "Epoch 98/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9499\n",
      "Epoch 99/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9445\n",
      "Epoch 100/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.9435\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.0577\n",
      "MSE: 1.058, RMSE: 1.028\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from math import sqrt  \n",
    "\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(21,activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8,activation='relu',  kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#fit the model \n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "#evaluating the model\n",
    "error = model.evaluate(X_test, Y_test)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.885647  ],\n",
       "       [2.6882758 ],\n",
       "       [3.2357147 ],\n",
       "       [2.0027611 ],\n",
       "       [1.2004082 ],\n",
       "       [1.9072399 ],\n",
       "       [1.7111577 ],\n",
       "       [1.4609259 ],\n",
       "       [1.9116541 ],\n",
       "       [2.0410013 ],\n",
       "       [2.157401  ],\n",
       "       [2.1434484 ],\n",
       "       [2.308308  ],\n",
       "       [1.8090173 ],\n",
       "       [1.7964625 ],\n",
       "       [4.0798726 ],\n",
       "       [2.2848752 ],\n",
       "       [3.2957823 ],\n",
       "       [1.9599739 ],\n",
       "       [2.465344  ],\n",
       "       [2.0589602 ],\n",
       "       [2.2867362 ],\n",
       "       [1.4913226 ],\n",
       "       [2.24923   ],\n",
       "       [2.2663128 ],\n",
       "       [1.1420709 ],\n",
       "       [2.687688  ],\n",
       "       [2.0257587 ],\n",
       "       [2.2398798 ],\n",
       "       [1.1605757 ],\n",
       "       [2.3515306 ],\n",
       "       [2.3475106 ],\n",
       "       [2.0420408 ],\n",
       "       [2.0199966 ],\n",
       "       [2.154678  ],\n",
       "       [2.3291998 ],\n",
       "       [2.1465695 ],\n",
       "       [1.354709  ],\n",
       "       [2.4986122 ],\n",
       "       [2.2505774 ],\n",
       "       [2.1145725 ],\n",
       "       [2.6126032 ],\n",
       "       [1.1561507 ],\n",
       "       [2.2007709 ],\n",
       "       [2.0105293 ],\n",
       "       [3.9162054 ],\n",
       "       [2.1504624 ],\n",
       "       [2.1047628 ],\n",
       "       [1.5443414 ],\n",
       "       [1.994788  ],\n",
       "       [2.8440354 ],\n",
       "       [1.4456259 ],\n",
       "       [1.915016  ],\n",
       "       [2.1262531 ],\n",
       "       [1.8809565 ],\n",
       "       [1.5488163 ],\n",
       "       [2.902829  ],\n",
       "       [1.1044418 ],\n",
       "       [1.1530579 ],\n",
       "       [2.1097662 ],\n",
       "       [4.2141294 ],\n",
       "       [1.5631951 ],\n",
       "       [2.2702997 ],\n",
       "       [2.2454407 ],\n",
       "       [3.552401  ],\n",
       "       [2.2815824 ],\n",
       "       [1.5703567 ],\n",
       "       [1.9353986 ],\n",
       "       [1.5591503 ],\n",
       "       [1.5711088 ],\n",
       "       [2.0692918 ],\n",
       "       [2.6340508 ],\n",
       "       [3.043531  ],\n",
       "       [2.125807  ],\n",
       "       [1.9639348 ],\n",
       "       [2.273375  ],\n",
       "       [1.7119266 ],\n",
       "       [1.9994179 ],\n",
       "       [2.046301  ],\n",
       "       [1.6721044 ],\n",
       "       [2.1951697 ],\n",
       "       [3.0483935 ],\n",
       "       [2.1086495 ],\n",
       "       [2.4865396 ],\n",
       "       [2.080968  ],\n",
       "       [1.6521212 ],\n",
       "       [2.6303241 ],\n",
       "       [2.5832484 ],\n",
       "       [1.8822759 ],\n",
       "       [2.466099  ],\n",
       "       [1.7655958 ],\n",
       "       [3.0625637 ],\n",
       "       [2.9246087 ],\n",
       "       [2.131748  ],\n",
       "       [3.015969  ],\n",
       "       [2.8743737 ],\n",
       "       [1.9998655 ],\n",
       "       [1.7656506 ],\n",
       "       [2.6683109 ],\n",
       "       [2.964167  ],\n",
       "       [2.6240926 ],\n",
       "       [2.8734198 ],\n",
       "       [1.6721003 ],\n",
       "       [2.9491432 ],\n",
       "       [1.5251458 ],\n",
       "       [3.0577    ],\n",
       "       [1.0566326 ],\n",
       "       [2.2041767 ],\n",
       "       [2.8584518 ],\n",
       "       [2.2171395 ],\n",
       "       [2.750274  ],\n",
       "       [2.413612  ],\n",
       "       [2.1743414 ],\n",
       "       [2.915007  ],\n",
       "       [2.161446  ],\n",
       "       [1.552371  ],\n",
       "       [2.71682   ],\n",
       "       [2.0523305 ],\n",
       "       [2.1043463 ],\n",
       "       [1.1248808 ],\n",
       "       [2.1706338 ],\n",
       "       [2.600139  ],\n",
       "       [2.5308793 ],\n",
       "       [2.9542263 ],\n",
       "       [2.262526  ],\n",
       "       [1.8935889 ],\n",
       "       [2.4684885 ],\n",
       "       [2.5379226 ],\n",
       "       [1.9095393 ],\n",
       "       [1.9087702 ],\n",
       "       [2.8829217 ],\n",
       "       [1.9429758 ],\n",
       "       [2.2552354 ],\n",
       "       [1.7542821 ],\n",
       "       [2.1855838 ],\n",
       "       [1.6343842 ],\n",
       "       [1.9502724 ],\n",
       "       [2.0257957 ],\n",
       "       [1.462607  ],\n",
       "       [1.7961646 ],\n",
       "       [1.1767312 ],\n",
       "       [2.088373  ],\n",
       "       [2.086658  ],\n",
       "       [2.0397823 ],\n",
       "       [2.112719  ],\n",
       "       [2.011862  ],\n",
       "       [1.7533783 ],\n",
       "       [2.2570984 ],\n",
       "       [1.1117867 ],\n",
       "       [2.3500605 ],\n",
       "       [1.8370503 ],\n",
       "       [0.7157699 ],\n",
       "       [1.9750338 ],\n",
       "       [2.6609132 ],\n",
       "       [2.2325907 ],\n",
       "       [1.195393  ],\n",
       "       [3.248264  ],\n",
       "       [2.4269524 ],\n",
       "       [3.5334263 ],\n",
       "       [1.5834411 ],\n",
       "       [0.7653712 ],\n",
       "       [1.5074438 ],\n",
       "       [2.2478147 ],\n",
       "       [1.7836167 ],\n",
       "       [2.6809156 ],\n",
       "       [1.2814144 ],\n",
       "       [2.264295  ],\n",
       "       [2.3416364 ],\n",
       "       [1.7872132 ],\n",
       "       [3.099749  ],\n",
       "       [1.9024765 ],\n",
       "       [2.5421355 ],\n",
       "       [2.2944086 ],\n",
       "       [1.9005702 ],\n",
       "       [1.5061699 ],\n",
       "       [2.484245  ],\n",
       "       [2.3076556 ],\n",
       "       [2.3165252 ],\n",
       "       [2.422978  ],\n",
       "       [2.0327933 ],\n",
       "       [1.6446745 ],\n",
       "       [2.531404  ],\n",
       "       [1.3335475 ],\n",
       "       [2.111737  ],\n",
       "       [2.1463945 ],\n",
       "       [3.294821  ],\n",
       "       [2.2077482 ],\n",
       "       [4.1532884 ],\n",
       "       [2.2586024 ],\n",
       "       [2.3962114 ],\n",
       "       [2.0834744 ],\n",
       "       [2.348838  ],\n",
       "       [1.748319  ],\n",
       "       [1.7319767 ],\n",
       "       [2.4079869 ],\n",
       "       [2.8180716 ],\n",
       "       [1.888062  ],\n",
       "       [2.007333  ],\n",
       "       [1.9307674 ],\n",
       "       [1.3207211 ],\n",
       "       [2.9283345 ],\n",
       "       [2.3111548 ],\n",
       "       [2.137191  ],\n",
       "       [2.2586768 ],\n",
       "       [1.1078779 ],\n",
       "       [2.2594144 ],\n",
       "       [1.8157328 ],\n",
       "       [1.9282812 ],\n",
       "       [1.7484165 ],\n",
       "       [2.3970487 ],\n",
       "       [2.4381335 ],\n",
       "       [3.0864024 ],\n",
       "       [2.2163973 ],\n",
       "       [1.9171883 ],\n",
       "       [2.2447128 ],\n",
       "       [1.8194717 ],\n",
       "       [2.1390243 ],\n",
       "       [1.5406102 ],\n",
       "       [3.1446936 ],\n",
       "       [2.0098374 ],\n",
       "       [2.1202934 ],\n",
       "       [1.4352981 ],\n",
       "       [2.4879656 ],\n",
       "       [2.7780058 ],\n",
       "       [1.6235012 ],\n",
       "       [1.346556  ],\n",
       "       [2.5357547 ],\n",
       "       [2.4070547 ],\n",
       "       [2.4934013 ],\n",
       "       [1.6411594 ],\n",
       "       [1.3605081 ],\n",
       "       [1.5321951 ],\n",
       "       [1.8735125 ],\n",
       "       [2.0447874 ],\n",
       "       [2.1732523 ],\n",
       "       [0.82359755],\n",
       "       [1.268329  ],\n",
       "       [1.6446222 ],\n",
       "       [1.7952701 ],\n",
       "       [2.0133739 ],\n",
       "       [2.0969825 ],\n",
       "       [3.0451188 ],\n",
       "       [4.400818  ],\n",
       "       [1.9419945 ],\n",
       "       [1.8065696 ],\n",
       "       [3.4985871 ],\n",
       "       [2.1663451 ],\n",
       "       [2.375231  ],\n",
       "       [2.3049734 ],\n",
       "       [2.8608637 ],\n",
       "       [0.8707359 ],\n",
       "       [2.758601  ],\n",
       "       [2.6812103 ],\n",
       "       [3.0728705 ],\n",
       "       [2.454719  ],\n",
       "       [2.4880211 ],\n",
       "       [2.377966  ],\n",
       "       [1.7816819 ],\n",
       "       [3.7792463 ],\n",
       "       [2.1430218 ],\n",
       "       [2.6313243 ],\n",
       "       [1.3894254 ],\n",
       "       [1.91151   ],\n",
       "       [1.7466877 ],\n",
       "       [1.903703  ],\n",
       "       [1.1222051 ],\n",
       "       [2.4877703 ],\n",
       "       [2.137203  ],\n",
       "       [1.5161446 ],\n",
       "       [2.564367  ],\n",
       "       [1.6724361 ],\n",
       "       [1.5371728 ],\n",
       "       [2.9272192 ],\n",
       "       [2.3448813 ],\n",
       "       [2.127665  ],\n",
       "       [1.3390939 ],\n",
       "       [2.5037296 ],\n",
       "       [2.0720317 ],\n",
       "       [1.4450217 ],\n",
       "       [1.8287212 ],\n",
       "       [0.926406  ],\n",
       "       [1.9135467 ],\n",
       "       [1.895701  ],\n",
       "       [2.148084  ],\n",
       "       [2.7638862 ],\n",
       "       [2.0223665 ],\n",
       "       [1.7833518 ],\n",
       "       [2.3294995 ],\n",
       "       [1.8956573 ],\n",
       "       [2.7154596 ],\n",
       "       [2.7730935 ],\n",
       "       [2.478303  ],\n",
       "       [2.697381  ],\n",
       "       [2.2222826 ],\n",
       "       [1.3995625 ],\n",
       "       [2.1463478 ],\n",
       "       [2.2099743 ],\n",
       "       [2.8492742 ],\n",
       "       [2.1383724 ],\n",
       "       [1.3472482 ],\n",
       "       [2.9026423 ],\n",
       "       [2.1408086 ],\n",
       "       [3.1973174 ],\n",
       "       [2.379215  ],\n",
       "       [2.476968  ],\n",
       "       [2.370306  ],\n",
       "       [2.5144165 ],\n",
       "       [2.2072015 ],\n",
       "       [1.2389709 ],\n",
       "       [2.2243483 ],\n",
       "       [2.2542753 ],\n",
       "       [2.2739928 ],\n",
       "       [2.9652956 ],\n",
       "       [2.0669131 ],\n",
       "       [1.6692    ],\n",
       "       [1.8514262 ],\n",
       "       [2.4067094 ],\n",
       "       [1.8520843 ],\n",
       "       [2.486353  ],\n",
       "       [2.4027383 ],\n",
       "       [2.1993315 ],\n",
       "       [2.7025132 ],\n",
       "       [2.4326181 ],\n",
       "       [1.7059919 ],\n",
       "       [2.0366254 ],\n",
       "       [1.8689752 ],\n",
       "       [2.2908003 ],\n",
       "       [1.7449566 ],\n",
       "       [1.8094138 ],\n",
       "       [2.0601807 ],\n",
       "       [2.0853004 ],\n",
       "       [3.83275   ],\n",
       "       [1.1261921 ],\n",
       "       [2.3277476 ],\n",
       "       [1.8825488 ],\n",
       "       [1.9381337 ],\n",
       "       [3.001931  ],\n",
       "       [2.2924721 ],\n",
       "       [2.006768  ],\n",
       "       [1.1142613 ],\n",
       "       [2.3342237 ],\n",
       "       [2.4299018 ],\n",
       "       [2.1314678 ],\n",
       "       [2.61003   ],\n",
       "       [2.0301719 ],\n",
       "       [1.6029122 ],\n",
       "       [1.9429902 ],\n",
       "       [2.000714  ],\n",
       "       [2.8673027 ],\n",
       "       [2.3727415 ],\n",
       "       [2.3518286 ],\n",
       "       [1.6808461 ],\n",
       "       [1.9408404 ],\n",
       "       [1.4893438 ],\n",
       "       [2.0997384 ],\n",
       "       [2.1984465 ],\n",
       "       [1.8818108 ],\n",
       "       [0.9876565 ],\n",
       "       [1.823685  ],\n",
       "       [2.3849127 ],\n",
       "       [2.1112456 ],\n",
       "       [1.8020748 ],\n",
       "       [2.0645998 ],\n",
       "       [2.7653618 ],\n",
       "       [1.871812  ],\n",
       "       [2.4794972 ],\n",
       "       [2.1501722 ],\n",
       "       [2.3845906 ],\n",
       "       [3.1605964 ],\n",
       "       [2.0765789 ],\n",
       "       [1.3962941 ],\n",
       "       [2.2608292 ],\n",
       "       [2.4587762 ],\n",
       "       [1.8319925 ],\n",
       "       [2.496592  ],\n",
       "       [1.886172  ],\n",
       "       [2.9562752 ],\n",
       "       [2.1963136 ],\n",
       "       [2.022882  ],\n",
       "       [1.9376136 ],\n",
       "       [2.7020967 ],\n",
       "       [1.8413776 ],\n",
       "       [1.8922302 ],\n",
       "       [1.9489392 ],\n",
       "       [2.1716511 ],\n",
       "       [1.6094035 ],\n",
       "       [3.0114162 ],\n",
       "       [1.8883537 ],\n",
       "       [1.8062637 ],\n",
       "       [2.1616168 ],\n",
       "       [2.5794609 ],\n",
       "       [1.9993527 ],\n",
       "       [1.6884605 ],\n",
       "       [1.4196914 ],\n",
       "       [2.1307867 ],\n",
       "       [1.656319  ],\n",
       "       [2.8787825 ],\n",
       "       [2.532212  ],\n",
       "       [2.123925  ],\n",
       "       [2.0268645 ],\n",
       "       [2.1084127 ],\n",
       "       [2.5829632 ],\n",
       "       [2.1132052 ],\n",
       "       [3.6238809 ],\n",
       "       [2.0426018 ],\n",
       "       [2.4170177 ],\n",
       "       [2.5319812 ],\n",
       "       [3.107688  ],\n",
       "       [2.180751  ],\n",
       "       [1.7840055 ],\n",
       "       [1.7333125 ],\n",
       "       [2.1379259 ],\n",
       "       [1.5827413 ],\n",
       "       [2.2159839 ],\n",
       "       [2.395863  ],\n",
       "       [1.6105412 ],\n",
       "       [2.2937968 ],\n",
       "       [1.903513  ],\n",
       "       [2.3826973 ],\n",
       "       [2.5856283 ],\n",
       "       [2.4587781 ],\n",
       "       [1.8499032 ],\n",
       "       [2.4907677 ],\n",
       "       [2.195323  ],\n",
       "       [1.8319663 ],\n",
       "       [2.5486522 ],\n",
       "       [2.1639535 ],\n",
       "       [1.9135565 ],\n",
       "       [2.733247  ],\n",
       "       [3.0815668 ],\n",
       "       [1.6775297 ],\n",
       "       [2.157486  ],\n",
       "       [0.9502914 ],\n",
       "       [3.0055232 ],\n",
       "       [3.0548365 ],\n",
       "       [2.0696127 ],\n",
       "       [1.8503631 ],\n",
       "       [2.2427223 ],\n",
       "       [1.3140992 ],\n",
       "       [2.3593318 ],\n",
       "       [1.6970179 ],\n",
       "       [2.8990657 ],\n",
       "       [2.2859638 ],\n",
       "       [1.2663993 ],\n",
       "       [1.8261639 ],\n",
       "       [1.9865091 ],\n",
       "       [2.3320167 ],\n",
       "       [3.4570317 ],\n",
       "       [1.6072735 ],\n",
       "       [2.1216805 ],\n",
       "       [2.2224138 ],\n",
       "       [2.5411608 ],\n",
       "       [2.7686791 ],\n",
       "       [2.297072  ],\n",
       "       [2.182908  ],\n",
       "       [1.9305522 ],\n",
       "       [2.3825016 ],\n",
       "       [2.6162107 ],\n",
       "       [2.0587919 ],\n",
       "       [1.1217788 ],\n",
       "       [1.4723023 ],\n",
       "       [1.7685698 ],\n",
       "       [1.6005409 ],\n",
       "       [1.7432481 ],\n",
       "       [2.0904603 ],\n",
       "       [1.6537455 ],\n",
       "       [3.333484  ],\n",
       "       [2.2014728 ],\n",
       "       [2.4087284 ],\n",
       "       [2.0585413 ],\n",
       "       [1.9945353 ],\n",
       "       [2.5897639 ],\n",
       "       [1.7659181 ],\n",
       "       [2.2060368 ],\n",
       "       [2.1614642 ],\n",
       "       [2.2290154 ],\n",
       "       [2.523939  ],\n",
       "       [1.6582667 ],\n",
       "       [2.2303433 ],\n",
       "       [2.925709  ],\n",
       "       [1.0421368 ],\n",
       "       [2.6224663 ],\n",
       "       [2.237276  ],\n",
       "       [1.5594656 ],\n",
       "       [1.7711583 ],\n",
       "       [2.0166326 ],\n",
       "       [2.2860453 ],\n",
       "       [3.6545968 ],\n",
       "       [1.5609902 ],\n",
       "       [3.116813  ],\n",
       "       [1.9524246 ],\n",
       "       [2.1921103 ],\n",
       "       [0.6990839 ],\n",
       "       [2.2927537 ],\n",
       "       [1.2686485 ],\n",
       "       [2.9286094 ],\n",
       "       [2.520159  ],\n",
       "       [1.8124859 ],\n",
       "       [1.9095105 ],\n",
       "       [2.3101475 ],\n",
       "       [2.256031  ],\n",
       "       [2.390951  ],\n",
       "       [2.9174104 ],\n",
       "       [2.643252  ],\n",
       "       [1.5240495 ],\n",
       "       [2.09268   ],\n",
       "       [2.2593782 ],\n",
       "       [1.772137  ],\n",
       "       [2.3245997 ],\n",
       "       [2.5946581 ],\n",
       "       [2.6813009 ],\n",
       "       [2.4430747 ],\n",
       "       [1.7939156 ],\n",
       "       [2.3429863 ],\n",
       "       [2.8297155 ],\n",
       "       [2.999334  ],\n",
       "       [2.6125524 ],\n",
       "       [2.0975034 ],\n",
       "       [1.5047687 ],\n",
       "       [1.7222214 ],\n",
       "       [1.8264078 ],\n",
       "       [0.872301  ],\n",
       "       [3.0658286 ],\n",
       "       [2.3453352 ],\n",
       "       [1.6217048 ],\n",
       "       [1.5318488 ],\n",
       "       [2.473174  ],\n",
       "       [1.9552385 ],\n",
       "       [2.3832452 ],\n",
       "       [2.3377087 ],\n",
       "       [2.1576464 ],\n",
       "       [2.1643546 ],\n",
       "       [2.7235157 ],\n",
       "       [3.4027488 ],\n",
       "       [2.546727  ],\n",
       "       [1.8674127 ],\n",
       "       [2.1652422 ],\n",
       "       [2.323343  ],\n",
       "       [2.264774  ],\n",
       "       [2.95624   ],\n",
       "       [1.6413141 ],\n",
       "       [2.398795  ],\n",
       "       [2.3635094 ],\n",
       "       [1.2702097 ],\n",
       "       [2.2524936 ],\n",
       "       [2.2713864 ],\n",
       "       [2.8828928 ],\n",
       "       [2.7725174 ],\n",
       "       [2.0153162 ],\n",
       "       [2.087302  ],\n",
       "       [2.2677422 ],\n",
       "       [1.7876049 ],\n",
       "       [2.0545785 ],\n",
       "       [2.939679  ],\n",
       "       [2.443724  ],\n",
       "       [2.1255155 ],\n",
       "       [1.3924831 ],\n",
       "       [2.0762177 ],\n",
       "       [2.0278022 ],\n",
       "       [2.379389  ],\n",
       "       [2.6164181 ],\n",
       "       [2.679167  ],\n",
       "       [2.9682899 ],\n",
       "       [1.9797126 ],\n",
       "       [2.2909315 ],\n",
       "       [2.612458  ],\n",
       "       [2.6287239 ],\n",
       "       [2.130139  ],\n",
       "       [1.6537821 ],\n",
       "       [1.1183913 ],\n",
       "       [1.9546452 ],\n",
       "       [2.2144706 ],\n",
       "       [2.3554373 ],\n",
       "       [1.8990294 ],\n",
       "       [2.3303812 ],\n",
       "       [2.0361497 ],\n",
       "       [2.4860327 ],\n",
       "       [2.1302617 ],\n",
       "       [2.4089103 ],\n",
       "       [1.8656694 ],\n",
       "       [0.7602562 ],\n",
       "       [2.874215  ],\n",
       "       [1.8383383 ],\n",
       "       [2.623928  ],\n",
       "       [1.7114655 ],\n",
       "       [1.405409  ],\n",
       "       [3.01925   ],\n",
       "       [1.5826536 ],\n",
       "       [2.3448718 ],\n",
       "       [2.3447168 ],\n",
       "       [1.3903935 ],\n",
       "       [3.0844967 ],\n",
       "       [1.9645075 ],\n",
       "       [2.2183175 ],\n",
       "       [2.0689287 ],\n",
       "       [2.5637145 ],\n",
       "       [2.5639265 ],\n",
       "       [2.5595012 ],\n",
       "       [2.297825  ],\n",
       "       [2.571182  ],\n",
       "       [2.4863937 ],\n",
       "       [2.0874853 ],\n",
       "       [2.5215788 ],\n",
       "       [1.9786867 ],\n",
       "       [1.9828479 ],\n",
       "       [1.7534064 ],\n",
       "       [2.0574446 ],\n",
       "       [2.351874  ],\n",
       "       [1.8189124 ],\n",
       "       [1.2254881 ],\n",
       "       [0.9599174 ],\n",
       "       [4.4599385 ],\n",
       "       [2.0357194 ],\n",
       "       [2.801678  ],\n",
       "       [2.1784694 ],\n",
       "       [1.5968379 ],\n",
       "       [1.8133302 ],\n",
       "       [1.9301313 ],\n",
       "       [1.764342  ],\n",
       "       [2.453129  ],\n",
       "       [2.9689746 ],\n",
       "       [2.0776217 ],\n",
       "       [2.9809988 ],\n",
       "       [2.085939  ],\n",
       "       [1.8595948 ],\n",
       "       [2.2833538 ],\n",
       "       [2.6052368 ],\n",
       "       [2.0106397 ],\n",
       "       [1.9466358 ],\n",
       "       [2.3724244 ],\n",
       "       [2.5500112 ],\n",
       "       [2.1423512 ],\n",
       "       [2.0884624 ],\n",
       "       [2.2595196 ],\n",
       "       [2.789702  ],\n",
       "       [1.6105775 ],\n",
       "       [0.9254495 ],\n",
       "       [1.6158568 ],\n",
       "       [1.1848334 ],\n",
       "       [1.5250181 ],\n",
       "       [1.7930566 ],\n",
       "       [2.1916368 ],\n",
       "       [2.3347404 ],\n",
       "       [2.809733  ],\n",
       "       [2.4902644 ],\n",
       "       [2.1111867 ],\n",
       "       [2.0894747 ],\n",
       "       [2.4531233 ],\n",
       "       [2.2619193 ],\n",
       "       [1.817479  ],\n",
       "       [2.1064594 ],\n",
       "       [1.1191596 ],\n",
       "       [1.9301113 ],\n",
       "       [1.8938677 ],\n",
       "       [2.606158  ],\n",
       "       [2.2194774 ],\n",
       "       [0.97275245],\n",
       "       [2.5297842 ],\n",
       "       [2.5646956 ],\n",
       "       [3.3415062 ],\n",
       "       [3.3535519 ],\n",
       "       [2.9627972 ],\n",
       "       [2.5296159 ],\n",
       "       [1.8752387 ],\n",
       "       [1.7842642 ],\n",
       "       [2.309955  ],\n",
       "       [2.0008883 ],\n",
       "       [1.1992491 ],\n",
       "       [1.5642848 ],\n",
       "       [1.169505  ],\n",
       "       [2.177956  ],\n",
       "       [1.2828782 ],\n",
       "       [2.1308222 ],\n",
       "       [2.5438385 ],\n",
       "       [3.0689323 ],\n",
       "       [2.4471161 ],\n",
       "       [2.8428435 ],\n",
       "       [2.194869  ],\n",
       "       [2.0728905 ],\n",
       "       [2.7924387 ],\n",
       "       [1.8753988 ],\n",
       "       [2.4817739 ],\n",
       "       [1.5083185 ],\n",
       "       [1.8715106 ],\n",
       "       [1.6048776 ],\n",
       "       [2.2227137 ],\n",
       "       [2.4616578 ],\n",
       "       [1.9872127 ],\n",
       "       [1.2831137 ],\n",
       "       [1.3940636 ],\n",
       "       [1.8877431 ],\n",
       "       [0.9090587 ],\n",
       "       [1.6785035 ],\n",
       "       [1.9123362 ],\n",
       "       [2.3478663 ],\n",
       "       [1.586491  ],\n",
       "       [1.5676291 ],\n",
       "       [1.7153107 ],\n",
       "       [2.7320771 ],\n",
       "       [1.9016714 ],\n",
       "       [2.293722  ],\n",
       "       [1.4999739 ],\n",
       "       [1.6251615 ],\n",
       "       [2.2344027 ],\n",
       "       [2.9741688 ],\n",
       "       [2.4028106 ],\n",
       "       [1.8690351 ],\n",
       "       [1.2891983 ],\n",
       "       [1.4648131 ],\n",
       "       [2.1599584 ],\n",
       "       [2.1903305 ],\n",
       "       [1.8262905 ],\n",
       "       [2.5015888 ],\n",
       "       [1.9683057 ],\n",
       "       [2.4904573 ],\n",
       "       [2.6688044 ],\n",
       "       [2.1936994 ],\n",
       "       [1.813643  ],\n",
       "       [1.6269104 ],\n",
       "       [2.7984757 ],\n",
       "       [2.1330807 ],\n",
       "       [2.1747217 ],\n",
       "       [2.0540452 ],\n",
       "       [1.8331361 ],\n",
       "       [2.1370158 ],\n",
       "       [2.7592921 ],\n",
       "       [1.2042207 ],\n",
       "       [1.6552047 ],\n",
       "       [1.4973718 ],\n",
       "       [1.6351609 ],\n",
       "       [2.3057482 ],\n",
       "       [2.8004227 ],\n",
       "       [2.1324017 ],\n",
       "       [2.109746  ],\n",
       "       [1.9731804 ],\n",
       "       [2.48799   ],\n",
       "       [1.5322236 ],\n",
       "       [2.0079715 ],\n",
       "       [2.3714454 ],\n",
       "       [2.2141538 ],\n",
       "       [1.9631032 ],\n",
       "       [2.5675733 ],\n",
       "       [3.0774553 ],\n",
       "       [1.1650275 ],\n",
       "       [3.0874653 ],\n",
       "       [3.2710044 ],\n",
       "       [0.8609644 ],\n",
       "       [1.7779685 ],\n",
       "       [1.0697817 ],\n",
       "       [1.3380262 ],\n",
       "       [2.9481046 ],\n",
       "       [1.4937656 ],\n",
       "       [2.3983545 ],\n",
       "       [2.1080687 ],\n",
       "       [2.624482  ],\n",
       "       [0.95233953],\n",
       "       [2.081218  ],\n",
       "       [2.535598  ],\n",
       "       [2.7892098 ],\n",
       "       [2.1897366 ],\n",
       "       [1.100334  ],\n",
       "       [1.0364352 ],\n",
       "       [2.8720489 ],\n",
       "       [2.8304417 ],\n",
       "       [2.3496478 ],\n",
       "       [2.675462  ],\n",
       "       [0.81411636],\n",
       "       [2.09462   ],\n",
       "       [2.4009242 ],\n",
       "       [2.9406118 ],\n",
       "       [2.0898879 ],\n",
       "       [1.4207191 ],\n",
       "       [2.288151  ],\n",
       "       [2.4231465 ],\n",
       "       [1.3798461 ],\n",
       "       [3.008842  ],\n",
       "       [2.3008368 ],\n",
       "       [2.2434719 ],\n",
       "       [2.4114356 ],\n",
       "       [3.7707675 ],\n",
       "       [2.3926163 ],\n",
       "       [2.0212355 ],\n",
       "       [1.5118464 ],\n",
       "       [1.8701127 ],\n",
       "       [1.7769083 ],\n",
       "       [2.3910189 ],\n",
       "       [2.1078787 ],\n",
       "       [1.2804548 ],\n",
       "       [2.0265894 ],\n",
       "       [2.1435292 ],\n",
       "       [1.5384954 ],\n",
       "       [2.2480388 ],\n",
       "       [2.727603  ],\n",
       "       [2.0029063 ],\n",
       "       [2.099503  ],\n",
       "       [2.430136  ],\n",
       "       [1.4339293 ],\n",
       "       [2.8107722 ],\n",
       "       [2.0584881 ],\n",
       "       [2.1153116 ],\n",
       "       [2.1028328 ],\n",
       "       [2.4833174 ],\n",
       "       [1.9438545 ],\n",
       "       [2.1687486 ],\n",
       "       [3.214114  ],\n",
       "       [1.9082701 ],\n",
       "       [2.2652788 ],\n",
       "       [2.1459506 ],\n",
       "       [1.9464234 ],\n",
       "       [1.459357  ],\n",
       "       [2.089709  ],\n",
       "       [2.252574  ],\n",
       "       [1.4958749 ],\n",
       "       [1.467888  ],\n",
       "       [1.9938061 ],\n",
       "       [0.8234652 ],\n",
       "       [3.197099  ],\n",
       "       [1.5765907 ],\n",
       "       [1.9705307 ],\n",
       "       [0.98940647]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prdecting using the test set \n",
    "yhatt = model.predict(X_test)\n",
    "yhatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.886\n"
     ]
    }
   ],
   "source": [
    "#predicting using a simple row\n",
    "i = 1299\n",
    "row = [df['% Iron Feed'].iloc[i],df['% Silica Feed'].iloc[i],df['Starch Flow'].iloc[i],df['Amina Flow'].iloc[i],df['Ore Pulp Flow'].iloc[i],df['Ore Pulp pH'].iloc[i],df['Ore Pulp Density'].iloc[i],df['p-Flotation Column 01 Air Flow'].iloc[i],df['p-Flotation Column 02 Air Flow'].iloc[i],df['p-Flotation Column 03 Air Flow'].iloc[i],df['p-Flotation Column 04 Air Flow'].iloc[i],df['p-Flotation Column 05 Air Flow'].iloc[i],df['p-Flotation Column 06 Air Flow'].iloc[i],df['p-Flotation Column 07 Air Flow'].iloc[i],df['p-Flotation Column 01 Level'].iloc[i],df['p-Flotation Column 02 Level'].iloc[i],df['p-Flotation Column 03 Level'].iloc[i],df['p-Flotation Column 04 Level'].iloc[i],df['p-Flotation Column 05 Level'].iloc[i],df['p-Flotation Column 06 Level'].iloc[i],df['p-Flotation Column 07 Level'].iloc[i]]\n",
    "row = min_max_scaler.transform([row])\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
